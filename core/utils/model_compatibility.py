"""
SOTAçº§æ¨¡å‹å…¼å®¹æ€§æ£€æŸ¥å’Œä¿®å¤å·¥å…·
è§£å†³æ¨¡å‹æ¶æ„ä¸åŒ¹é…ã€ç»´åº¦é”™è¯¯ç­‰é—®é¢˜

ä½œè€…ï¼šSOTAçº§ä¼˜åŒ–ç‰ˆæœ¬
"""

import torch
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Any


class ModelCompatibilityChecker:
    """SOTAçº§æ¨¡å‹å…¼å®¹æ€§æ£€æŸ¥å™¨"""
    
    def __init__(self, model, device: str = 'cuda'):
        self.model = model
        self.device = device
        
    def diagnose_model_architecture(self) -> Dict[str, Any]:
        """å…¨é¢è¯Šæ–­æ¨¡å‹æ¶æ„"""
        diagnosis = {
            'model_type': type(self.model).__name__,
            'dynamics_type': type(self.model.dynamics).__name__ if hasattr(self.model, 'dynamics') else None,
            'parameters': {},
            'methods': [],
            'embedding_layers': {},
            'issues': [],
            'recommendations': []
        }
        
        # æ£€æŸ¥dynamicsæ¨¡å—
        if hasattr(self.model, 'dynamics'):
            dynamics = self.model.dynamics
            
            # åŸºæœ¬å‚æ•°
            diagnosis['parameters']['num_classes'] = getattr(dynamics, 'num_classes', None)
            diagnosis['parameters']['num_bond_classes'] = getattr(dynamics, 'num_bond_classes', None)
            diagnosis['parameters']['time_emb_dim'] = getattr(dynamics, 'time_emb_dim', None)
            diagnosis['parameters']['bond_bfn'] = getattr(dynamics, 'bond_bfn', None)
            
            # æ£€æŸ¥åµŒå…¥å±‚
            if hasattr(dynamics, 'ligand_atom_emb'):
                emb_weight = dynamics.ligand_atom_emb.weight
                diagnosis['embedding_layers']['ligand_atom_emb'] = {
                    'shape': emb_weight.shape,
                    'input_dim': emb_weight.shape[1],
                    'output_dim': emb_weight.shape[0]
                }
            
            if hasattr(dynamics, 'protein_atom_emb'):
                emb_weight = dynamics.protein_atom_emb.weight
                diagnosis['embedding_layers']['protein_atom_emb'] = {
                    'shape': emb_weight.shape,
                    'input_dim': emb_weight.shape[1],
                    'output_dim': emb_weight.shape[0]
                }
            
            # æ£€æŸ¥æ—¶é—´åµŒå…¥
            if hasattr(dynamics, 'time_emb_layer'):
                diagnosis['parameters']['has_time_emb_layer'] = True
                if hasattr(dynamics.time_emb_layer, 'time_emb_dim'):
                    diagnosis['parameters']['time_emb_layer_dim'] = dynamics.time_emb_layer.time_emb_dim
            else:
                diagnosis['parameters']['has_time_emb_layer'] = False
            
            # æ£€æŸ¥å¯ç”¨æ–¹æ³•
            methods = ['forward', 'interdependency_modeling', 'loss_one_step', 'sample']
            for method in methods:
                if hasattr(dynamics, method):
                    diagnosis['methods'].append(method)
        
        # æ£€æŸ¥è®­ç»ƒç›¸å…³æ–¹æ³•
        if hasattr(self.model, 'training_step'):
            diagnosis['methods'].append('training_step')
        
        return diagnosis
    
    def check_input_compatibility(self, batch) -> Dict[str, Any]:
        """æ£€æŸ¥è¾“å…¥æ•°æ®ä¸æ¨¡å‹çš„å…¼å®¹æ€§"""
        compatibility = {
            'ligand_data': {},
            'protein_data': {},
            'issues': [],
            'fixes': []
        }
        
        # æ£€æŸ¥é…ä½“æ•°æ®
        if hasattr(batch, 'ligand_pos'):
            compatibility['ligand_data']['pos_shape'] = batch.ligand_pos.shape
        if hasattr(batch, 'ligand_atom_feature_full'):
            compatibility['ligand_data']['feature_shape'] = batch.ligand_atom_feature_full.shape
        if hasattr(batch, 'ligand_element'):
            compatibility['ligand_data']['element_shape'] = batch.ligand_element.shape
        
        # æ£€æŸ¥è›‹ç™½è´¨æ•°æ®
        if hasattr(batch, 'protein_pos'):
            compatibility['protein_data']['pos_shape'] = batch.protein_pos.shape
        if hasattr(batch, 'protein_atom_feature'):
            compatibility['protein_data']['feature_shape'] = batch.protein_atom_feature.shape
        
        # æ£€æŸ¥ç»´åº¦å…¼å®¹æ€§
        if hasattr(self.model, 'dynamics') and hasattr(self.model.dynamics, 'ligand_atom_emb'):
            expected_ligand_dim = self.model.dynamics.ligand_atom_emb.weight.shape[1]
            
            if hasattr(batch, 'ligand_atom_feature_full'):
                actual_ligand_dim = batch.ligand_atom_feature_full.shape[1]
                time_dim = getattr(self.model.dynamics, 'time_emb_dim', 1)
                
                total_expected = expected_ligand_dim
                total_actual = actual_ligand_dim + time_dim
                
                if total_actual != total_expected:
                    compatibility['issues'].append({
                        'type': 'dimension_mismatch',
                        'component': 'ligand_features',
                        'expected': total_expected,
                        'actual': total_actual,
                        'difference': total_actual - total_expected
                    })
                    
                    if total_actual > total_expected:
                        compatibility['fixes'].append({
                            'type': 'truncate',
                            'component': 'ligand_features',
                            'action': f'æˆªæ–­åˆ° {total_expected - time_dim} ç»´'
                        })
                    else:
                        compatibility['fixes'].append({
                            'type': 'pad',
                            'component': 'ligand_features',
                            'action': f'å¡«å……åˆ° {total_expected - time_dim} ç»´'
                        })
        
        return compatibility
    
    def fix_input_dimensions(self, batch, t_tensor) -> Tuple[Any, torch.Tensor]:
        """ä¿®å¤è¾“å…¥ç»´åº¦ä¸åŒ¹é…"""
        fixed_batch = batch
        
        if hasattr(self.model, 'dynamics') and hasattr(self.model.dynamics, 'ligand_atom_emb'):
            expected_ligand_dim = self.model.dynamics.ligand_atom_emb.weight.shape[1]
            time_dim = t_tensor.shape[1]
            
            if hasattr(batch, 'ligand_atom_feature_full'):
                ligand_v = batch.ligand_atom_feature_full
                current_dim = ligand_v.shape[1]
                target_dim = expected_ligand_dim - time_dim
                
                if current_dim != target_dim:
                    if current_dim > target_dim:
                        # æˆªæ–­
                        ligand_v_fixed = ligand_v[:, :target_dim]
                        print(f"ğŸ”§ æˆªæ–­é…ä½“ç‰¹å¾: {ligand_v.shape} -> {ligand_v_fixed.shape}")
                    else:
                        # å¡«å……
                        padding_dim = target_dim - current_dim
                        padding = torch.zeros(ligand_v.shape[0], padding_dim, 
                                            device=ligand_v.device, dtype=ligand_v.dtype)
                        ligand_v_fixed = torch.cat([ligand_v, padding], dim=1)
                        print(f"ğŸ”§ å¡«å……é…ä½“ç‰¹å¾: {ligand_v.shape} -> {ligand_v_fixed.shape}")
                    
                    # æ›´æ–°batchä¸­çš„ç‰¹å¾
                    fixed_batch.ligand_atom_feature_full = ligand_v_fixed
        
        return fixed_batch, t_tensor
    
    def suggest_compatible_method(self) -> str:
        """å»ºè®®æœ€å…¼å®¹çš„è°ƒç”¨æ–¹æ³•"""
        diagnosis = self.diagnose_model_architecture()
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        if 'loss_one_step' in diagnosis['methods']:
            return 'loss_one_step'
        elif 'training_step' in diagnosis['methods']:
            return 'training_step'
        elif 'interdependency_modeling' in diagnosis['methods']:
            return 'interdependency_modeling'
        elif 'forward' in diagnosis['methods']:
            return 'forward'
        else:
            return 'none'
    
    def create_compatible_call_kwargs(self, batch, t_tensor, conditions=None) -> Dict[str, Any]:
        """åˆ›å»ºå…¼å®¹çš„è°ƒç”¨å‚æ•°"""
        method = self.suggest_compatible_method()
        
        if method == 'loss_one_step':
            return self._create_loss_one_step_kwargs(batch, t_tensor, conditions)
        elif method == 'training_step':
            return self._create_training_step_kwargs(batch, conditions)
        elif method == 'interdependency_modeling':
            return self._create_interdependency_kwargs(batch, t_tensor, conditions)
        else:
            return {}
    
    def _create_loss_one_step_kwargs(self, batch, t_tensor, conditions=None) -> Dict[str, Any]:
        """åˆ›å»ºloss_one_stepè°ƒç”¨å‚æ•°"""
        kwargs = {
            't': t_tensor,
            'protein_pos': getattr(batch, 'protein_pos', None),
            'protein_v': getattr(batch, 'protein_atom_feature', None),
            'batch_protein': getattr(batch, 'protein_element_batch', None),
            'ligand_pos': getattr(batch, 'ligand_pos', None),
            'ligand_v': getattr(batch, 'ligand_atom_feature_full', None),
            'batch_ligand': getattr(batch, 'ligand_element_batch', None),
            'ligand_bond_type': getattr(batch, 'ligand_fc_bond_type', None),
            'ligand_bond_index': getattr(batch, 'ligand_fc_bond_index', None),
            'batch_ligand_bond': getattr(batch, 'ligand_fc_bond_type_batch', None),
            'include_protein': True,
            't_pos': t_tensor,
        }
        
        if conditions is not None:
            kwargs['conditions'] = conditions
        
        return kwargs
    
    def _create_training_step_kwargs(self, batch, conditions=None) -> Dict[str, Any]:
        """åˆ›å»ºtraining_stepè°ƒç”¨å‚æ•°"""
        return {'batch': batch, 'batch_idx': 0}
    
    def _create_interdependency_kwargs(self, batch, t_tensor, conditions=None) -> Dict[str, Any]:
        """åˆ›å»ºinterdependency_modelingè°ƒç”¨å‚æ•°"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹å®ç°
        return {}


def create_compatibility_checker(model, device: str = 'cuda') -> ModelCompatibilityChecker:
    """åˆ›å»ºæ¨¡å‹å…¼å®¹æ€§æ£€æŸ¥å™¨"""
    return ModelCompatibilityChecker(model, device)


def diagnose_model_compatibility(model, batch, device: str = 'cuda') -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šè¯Šæ–­æ¨¡å‹å…¼å®¹æ€§"""
    checker = ModelCompatibilityChecker(model, device)
    
    model_diagnosis = checker.diagnose_model_architecture()
    input_compatibility = checker.check_input_compatibility(batch)
    
    return {
        'model': model_diagnosis,
        'input': input_compatibility,
        'suggested_method': checker.suggest_compatible_method()
    }
