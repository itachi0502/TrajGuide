"""
SOTAçº§åˆ†å­æ¡ä»¶æ•°æ®é¢„å¤„ç†ç³»ç»Ÿ
==================================

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä¸ºæ¯ä¸ªè®­ç»ƒåˆ†å­è®¡ç®—QEDã€SAæ¡ä»¶ç‰¹å¾
2. ç»Ÿè®¡å‡å€¼/æ–¹å·®å¹¶å®ç°æ ‡å‡†åŒ–
3. ä¿å­˜åˆ°æ ‡æ³¨æ–‡ä»¶ï¼Œæ”¯æŒå¿«é€ŸåŠ è½½
4. ä¸ç°æœ‰MolPilotæ•°æ®æµæ— ç¼é›†æˆ

è®¾è®¡åŸåˆ™ï¼š
- é«˜æ•ˆï¼šæ”¯æŒæ‰¹é‡å¤„ç†å’Œç¼“å­˜æœºåˆ¶
- é²æ£’ï¼šå¤„ç†å¼‚å¸¸åˆ†å­å’Œè¾¹ç•Œæƒ…å†µ
- å¯æ‰©å±•ï¼šæ”¯æŒæ–°å¢æ¡ä»¶ç±»å‹
- å…¼å®¹ï¼šä¸ç°æœ‰æ•°æ®é›†æ ¼å¼å®Œå…¨å…¼å®¹
"""

import os
import json
import pickle
import numpy as np
import torch
from typing import Dict, List, Optional, Tuple, Union
from pathlib import Path
from tqdm import tqdm
import logging
from dataclasses import dataclass, asdict
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

try:
    # SOTA: æŠ‘åˆ¶RDKitçš„è°ƒè¯•è¾“å‡º
    import warnings
    warnings.filterwarnings('ignore')

    from rdkit import Chem
    from rdkit.Chem import Descriptors, QED, Crippen
    from rdkit.Contrib.SA_Score import sascorer
    from rdkit import RDLogger
    RDLogger.DisableLog('rdApp.*')  # æŠ‘åˆ¶RDKitæ—¥å¿—

    RDKIT_AVAILABLE = True
except ImportError:
    print("âš ï¸  RDKitæœªå®‰è£…ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¡ä»¶å€¼")
    RDKIT_AVAILABLE = False


@dataclass
class MolecularConditions:
    """åˆ†å­æ¡ä»¶æ•°æ®ç»“æ„"""
    qed: float          # Drug-likeness (0-1)
    sa: float           # Synthetic Accessibility (0-1, æ ‡å‡†åŒ–)

    def to_tensor(self) -> torch.Tensor:
        """è½¬æ¢ä¸ºå¼ é‡æ ¼å¼"""
        return torch.tensor([self.qed, self.sa], dtype=torch.float32)

    @classmethod
    def from_tensor(cls, tensor: torch.Tensor) -> 'MolecularConditions':
        """ä»å¼ é‡åˆ›å»ºæ¡ä»¶å¯¹è±¡"""
        return cls(
            qed=tensor[0].item(),
            sa=tensor[1].item()
        )


@dataclass
class ConditionStatistics:
    """æ¡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
    mean: Dict[str, float]
    std: Dict[str, float]
    min_val: Dict[str, float]
    max_val: Dict[str, float]
    count: int
    
    def normalize_conditions(self, conditions: MolecularConditions) -> MolecularConditions:
        """æ ‡å‡†åŒ–æ¡ä»¶"""
        return MolecularConditions(
            qed=conditions.qed,  # QEDå·²ç»åœ¨0-1èŒƒå›´å†…
            sa=min(conditions.sa, 10.0) / 10.0  # SAæ ‡å‡†åŒ–åˆ°0-1
        )


class MolecularConditionProcessor:
    """SOTAçº§åˆ†å­æ¡ä»¶å¤„ç†å™¨"""
    
    def __init__(self, cache_dir: str = "data/condition_cache",
                 enable_multiprocessing: bool = True,
                 max_workers: Optional[int] = None,
                 dataset_name: str = "default"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self.enable_multiprocessing = enable_multiprocessing
        self.max_workers = max_workers or min(8, mp.cpu_count())
        self.dataset_name = dataset_name

        # ç»Ÿè®¡ä¿¡æ¯
        self.statistics: Optional[ConditionStatistics] = None
        self.stats_file = self.cache_dir / f"{dataset_name}_condition_statistics.json"

        # SOTA: æŒä¹…åŒ–ç¼“å­˜æ–‡ä»¶ï¼ˆåŸºäºæ•°æ®é›†åç§°ï¼‰
        self.condition_cache_file = self.cache_dir / f"{dataset_name}_molecular_conditions.pkl"
        self.condition_cache: Dict[str, MolecularConditions] = {}

        # SOTA: æ•°æ®é›†çº§åˆ«çš„æ¡ä»¶æ˜ å°„æ–‡ä»¶
        self.dataset_condition_file = self.cache_dir / f"{dataset_name}_dataset_conditions.json"
        self.dataset_conditions: Dict[str, List[float]] = {}  # {sample_id: [qed, sa]}

        # åŠ è½½å·²æœ‰ç¼“å­˜
        self._load_cache()
        self._load_statistics()
        self._load_dataset_conditions()

        print(f"âœ… SOTAåˆ†å­æ¡ä»¶å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ•°æ®é›†: {dataset_name}")
        print(f"   ç¼“å­˜ç›®å½•: {self.cache_dir}")
        print(f"   å¤šè¿›ç¨‹: {self.enable_multiprocessing} (workers: {self.max_workers})")
        print(f"   å·²ç¼“å­˜SMILESæ¡ä»¶: {len(self.condition_cache)}")
        print(f"   å·²ç¼“å­˜æ•°æ®é›†æ¡ä»¶: {len(self.dataset_conditions)}")
    
    def _load_cache(self):
        """åŠ è½½æ¡ä»¶ç¼“å­˜"""
        if self.condition_cache_file.exists():
            try:
                with open(self.condition_cache_file, 'rb') as f:
                    self.condition_cache = pickle.load(f)
                print(f"ğŸ“Š åŠ è½½æ¡ä»¶ç¼“å­˜: {len(self.condition_cache)} ä¸ªåˆ†å­")
            except Exception as e:
                print(f"âš ï¸  æ¡ä»¶ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
                self.condition_cache = {}
    
    def _save_cache(self):
        """ä¿å­˜æ¡ä»¶ç¼“å­˜"""
        try:
            with open(self.condition_cache_file, 'wb') as f:
                pickle.dump(self.condition_cache, f)
            print(f"ğŸ’¾ æ¡ä»¶ç¼“å­˜å·²ä¿å­˜: {len(self.condition_cache)} ä¸ªåˆ†å­")
        except Exception as e:
            print(f"âš ï¸  æ¡ä»¶ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def _load_statistics(self):
        """åŠ è½½ç»Ÿè®¡ä¿¡æ¯"""
        if self.stats_file.exists():
            try:
                with open(self.stats_file, 'r') as f:
                    stats_dict = json.load(f)
                    self.statistics = ConditionStatistics(**stats_dict)
                print(f"ğŸ“ˆ åŠ è½½æ¡ä»¶ç»Ÿè®¡ä¿¡æ¯: {self.statistics.count} ä¸ªæ ·æœ¬")
            except Exception as e:
                print(f"âš ï¸  ç»Ÿè®¡ä¿¡æ¯åŠ è½½å¤±è´¥: {e}")
                self.statistics = None
    
    def _save_statistics(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.statistics is not None:
            try:
                with open(self.stats_file, 'w') as f:
                    json.dump(asdict(self.statistics), f, indent=2)
                print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {self.statistics.count} ä¸ªæ ·æœ¬")
            except Exception as e:
                print(f"âš ï¸  ç»Ÿè®¡ä¿¡æ¯ä¿å­˜å¤±è´¥: {e}")
    
    @staticmethod
    def _calculate_single_condition(smiles: str) -> Optional[MolecularConditions]:
        """è®¡ç®—å•ä¸ªåˆ†å­çš„æ¡ä»¶ï¼ˆé™æ€æ–¹æ³•ï¼Œæ”¯æŒå¤šè¿›ç¨‹ï¼‰"""
        if not RDKIT_AVAILABLE:
            return MolecularConditions(qed=0.5, sa=0.5)

        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol is None:
                return None

            # è®¡ç®—QEDå’ŒSA
            qed_value = QED.qed(mol)
            sa_value = sascorer.calculateScore(mol)

            return MolecularConditions(
                qed=qed_value,
                sa=sa_value
            )

        except Exception as e:
            print(f"âš ï¸  åˆ†å­æ¡ä»¶è®¡ç®—å¤±è´¥ {smiles}: {e}")
            return None
    
    def calculate_conditions(self, smiles: Union[str, List[str]], 
                           use_cache: bool = True) -> Union[MolecularConditions, List[MolecularConditions]]:
        """è®¡ç®—åˆ†å­æ¡ä»¶"""
        is_single = isinstance(smiles, str)
        smiles_list = [smiles] if is_single else smiles
        
        results = []
        to_calculate = []
        
        # æ£€æŸ¥ç¼“å­˜
        for smi in smiles_list:
            if use_cache and smi in self.condition_cache:
                results.append(self.condition_cache[smi])
            else:
                results.append(None)
                to_calculate.append((len(results) - 1, smi))
        
        # è®¡ç®—æœªç¼“å­˜çš„æ¡ä»¶
        if to_calculate:
            print(f"ğŸ“Š éœ€è¦è®¡ç®—çš„åˆ†å­æ•°: {len(to_calculate)}")
            print(f"ğŸ“Š ç»“æœåˆ—è¡¨é•¿åº¦: {len(results)}")

            if self.enable_multiprocessing and len(to_calculate) > 1:
                # SOTA: ä¿®å¤å¤šè¿›ç¨‹è®¡ç®—çš„ç´¢å¼•é”™è¯¯
                print(f"ğŸ”„ ä½¿ç”¨å¤šè¿›ç¨‹è®¡ç®— (workers: {self.max_workers})")
                with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
                    # åˆ›å»ºfutureåˆ°(ç»“æœç´¢å¼•, SMILES)çš„æ˜ å°„
                    future_to_data = {
                        executor.submit(self._calculate_single_condition, smi): (idx, smi)
                        for idx, smi in to_calculate
                    }
                    print(f"ğŸ“Š æäº¤çš„ä»»åŠ¡æ•°: {len(future_to_data)}")

                    for future in tqdm(as_completed(future_to_data),
                                     total=len(to_calculate),
                                     desc="è®¡ç®—åˆ†å­æ¡ä»¶"):
                        try:
                            # SOTA: å®‰å…¨è·å–æ˜ å°„æ•°æ®
                            if future not in future_to_data:
                                print(f"âš ï¸  Futureæ˜ å°„ä¸¢å¤±ï¼Œè·³è¿‡")
                                continue

                            idx, smi = future_to_data[future]

                            # éªŒè¯ç´¢å¼•æœ‰æ•ˆæ€§
                            if idx >= len(results):
                                print(f"âš ï¸  ç´¢å¼•è¶Šç•Œ {idx} >= {len(results)}ï¼Œè·³è¿‡")
                                continue

                            condition = future.result()
                            if condition is not None:
                                results[idx] = condition
                                if use_cache:
                                    self.condition_cache[smi] = condition
                            else:
                                # ä½¿ç”¨é»˜è®¤å€¼
                                default_condition = MolecularConditions(qed=0.5, sa=0.5)
                                results[idx] = default_condition
                                if use_cache:
                                    self.condition_cache[smi] = default_condition

                        except Exception as e:
                            print(f"âš ï¸  æ¡ä»¶è®¡ç®—å¼‚å¸¸: {e}")
                            # å°è¯•è·å–ç´¢å¼•ï¼Œå¦‚æœå¤±è´¥åˆ™è·³è¿‡
                            try:
                                idx, smi = future_to_data.get(future, (None, None))
                                if idx is not None and idx < len(results):
                                    default_condition = MolecularConditions(qed=0.5, sa=0.5)
                                    results[idx] = default_condition
                                    if use_cache:
                                        self.condition_cache[smi] = default_condition
                            except:
                                pass  # å¦‚æœæ— æ³•æ¢å¤ï¼Œè·³è¿‡è¿™ä¸ªæ ·æœ¬
            else:
                # SOTA: å•è¿›ç¨‹è®¡ç®—ï¼ˆå¸¦å®‰å…¨æ£€æŸ¥ï¼‰
                for idx, smi in tqdm(to_calculate, desc="è®¡ç®—åˆ†å­æ¡ä»¶"):
                    try:
                        # éªŒè¯ç´¢å¼•æœ‰æ•ˆæ€§
                        if idx >= len(results):
                            print(f"âš ï¸  å•è¿›ç¨‹ç´¢å¼•è¶Šç•Œ {idx} >= {len(results)}ï¼Œè·³è¿‡")
                            continue

                        condition = self._calculate_single_condition(smi)
                        if condition is not None:
                            results[idx] = condition
                            if use_cache:
                                self.condition_cache[smi] = condition
                        else:
                            # ä½¿ç”¨é»˜è®¤å€¼
                            default_condition = MolecularConditions(qed=0.5, sa=0.5)
                            results[idx] = default_condition
                            if use_cache:
                                self.condition_cache[smi] = default_condition
                    except Exception as e:
                        print(f"âš ï¸  å•è¿›ç¨‹æ¡ä»¶è®¡ç®—å¼‚å¸¸ (idx={idx}): {e}")
                        if idx < len(results):
                            default_condition = MolecularConditions(qed=0.5, sa=0.5)
                            results[idx] = default_condition
                            if use_cache:
                                self.condition_cache[smi] = default_condition
        
        # ä¿å­˜ç¼“å­˜
        if use_cache and to_calculate:
            self._save_cache()
        
        return results[0] if is_single else results

    def _load_dataset_conditions(self):
        """åŠ è½½æ•°æ®é›†çº§åˆ«çš„æ¡ä»¶æ˜ å°„"""
        if self.dataset_condition_file.exists():
            try:
                with open(self.dataset_condition_file, 'r') as f:
                    self.dataset_conditions = json.load(f)
                print(f"ğŸ“Š åŠ è½½æ•°æ®é›†æ¡ä»¶æ˜ å°„: {len(self.dataset_conditions)} ä¸ªæ ·æœ¬")
            except Exception as e:
                print(f"âš ï¸  æ•°æ®é›†æ¡ä»¶æ˜ å°„åŠ è½½å¤±è´¥: {e}")
                self.dataset_conditions = {}

    def _save_dataset_conditions(self):
        """ä¿å­˜æ•°æ®é›†çº§åˆ«çš„æ¡ä»¶æ˜ å°„"""
        try:
            with open(self.dataset_condition_file, 'w') as f:
                json.dump(self.dataset_conditions, f, indent=2)
            print(f"ğŸ’¾ æ•°æ®é›†æ¡ä»¶æ˜ å°„å·²ä¿å­˜: {len(self.dataset_conditions)} ä¸ªæ ·æœ¬")
        except Exception as e:
            print(f"âš ï¸  æ•°æ®é›†æ¡ä»¶æ˜ å°„ä¿å­˜å¤±è´¥: {e}")

    def precompute_dataset_conditions(self, dataset, sample_id_key: str = 'ligand_filename',
                                    smiles_key: str = 'ligand_smiles', force_recompute: bool = False):
        """SOTA: é¢„è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„æ¡ä»¶ï¼Œå»ºç«‹sample_idåˆ°æ¡ä»¶çš„æ˜ å°„"""
        print(f"ğŸ”„ é¢„è®¡ç®—æ•°æ®é›†æ¡ä»¶: {len(dataset)} ä¸ªæ ·æœ¬")

        if not force_recompute and len(self.dataset_conditions) >= len(dataset) * 0.9:
            print(f"ğŸ“Š æ•°æ®é›†æ¡ä»¶å·²é¢„è®¡ç®—ï¼Œè·³è¿‡")
            return

        # æ‰¹é‡æå–SMILES
        smiles_list = []
        sample_ids = []
        failed_count = 0

        print(f"ğŸ“Š å¼€å§‹æå–SMILESå’Œæ ·æœ¬ID...")

        # SOTA: æŠ‘åˆ¶å¯èƒ½çš„è°ƒè¯•è¾“å‡º
        import sys
        import os

        # ä¸´æ—¶é‡å®šå‘stderræ¥æŠ‘åˆ¶å¯èƒ½çš„è°ƒè¯•è¾“å‡º
        original_stderr = sys.stderr
        try:
            # åˆ›å»ºä¸€ä¸ªç©ºçš„æ–‡ä»¶å¯¹è±¡æ¥æŠ‘åˆ¶è¾“å‡º
            devnull = open(os.devnull, 'w')

            for i, data in enumerate(tqdm(dataset, desc="æå–SMILES")):
                try:
                    # ä¸´æ—¶æŠ‘åˆ¶stderrè¾“å‡º
                    sys.stderr = devnull

                    # è·å–æ ·æœ¬ID
                    sample_id = None
                    if hasattr(data, sample_id_key):
                        sample_id = getattr(data, sample_id_key)
                    elif hasattr(data, 'ligand_filename'):
                        sample_id = data.ligand_filename
                    elif hasattr(data, 'filename'):
                        sample_id = data.filename
                    else:
                        sample_id = f"sample_{i}"

                    # è·å–SMILES
                    smiles = None
                    if hasattr(data, smiles_key):
                        smiles = getattr(data, smiles_key)
                    elif hasattr(data, 'ligand_smiles'):
                        smiles = data.ligand_smiles
                    elif hasattr(data, 'smiles'):
                        smiles = data.smiles

                    # æ¢å¤stderr
                    sys.stderr = original_stderr

                    if smiles and smiles.strip():
                        smiles_list.append(smiles.strip())
                        sample_ids.append(sample_id)
                    else:
                        # å¦‚æœæ²¡æœ‰SMILESï¼Œä½¿ç”¨é»˜è®¤æ¡ä»¶
                        self.dataset_conditions[sample_id] = [0.5, 0.5]
                        failed_count += 1

                except Exception as e:
                    # æ¢å¤stderr
                    sys.stderr = original_stderr
                    # åªåœ¨çœŸæ­£éœ€è¦æ—¶è¾“å‡ºé”™è¯¯ä¿¡æ¯
                    if i % 1000 == 0:  # æ¯1000ä¸ªæ ·æœ¬è¾“å‡ºä¸€æ¬¡é”™è¯¯
                        print(f"âš ï¸  æ ·æœ¬ {i} æ•°æ®æå–å¤±è´¥: {e}")
                    sample_id = f"sample_{i}"
                    self.dataset_conditions[sample_id] = [0.5, 0.5]
                    failed_count += 1

        finally:
            # ç¡®ä¿æ¢å¤stderr
            sys.stderr = original_stderr
            devnull.close()

        print(f"ğŸ“Š SMILESæå–å®Œæˆ:")
        print(f"   æˆåŠŸæå–: {len(smiles_list)} ä¸ª")
        print(f"   ä½¿ç”¨é»˜è®¤: {failed_count} ä¸ª")

        # æ‰¹é‡è®¡ç®—æ¡ä»¶
        if smiles_list:
            print(f"ğŸ”„ æ‰¹é‡è®¡ç®— {len(smiles_list)} ä¸ªåˆ†å­çš„æ¡ä»¶")
            conditions_list = self.calculate_conditions(smiles_list, use_cache=True)

            # å»ºç«‹æ˜ å°„
            success_count = 0
            for sample_id, conditions in zip(sample_ids, conditions_list):
                if conditions is not None:
                    normalized = self.normalize_conditions(conditions)
                    self.dataset_conditions[sample_id] = [normalized.qed, normalized.sa]
                    success_count += 1
                else:
                    self.dataset_conditions[sample_id] = [0.5, 0.5]

            print(f"ğŸ“Š æ¡ä»¶è®¡ç®—å®Œæˆ:")
            print(f"   æˆåŠŸè®¡ç®—: {success_count} ä¸ª")
            print(f"   ä½¿ç”¨é»˜è®¤: {len(sample_ids) - success_count} ä¸ª")

        # ä¿å­˜æ˜ å°„
        self._save_dataset_conditions()
        print(f"âœ… æ•°æ®é›†æ¡ä»¶é¢„è®¡ç®—å®Œæˆ: {len(self.dataset_conditions)} ä¸ªæ ·æœ¬")

    def get_conditions_by_sample_id(self, sample_id: str) -> torch.Tensor:
        """æ ¹æ®æ ·æœ¬IDè·å–é¢„è®¡ç®—çš„æ¡ä»¶"""
        if sample_id in self.dataset_conditions:
            return torch.tensor(self.dataset_conditions[sample_id], dtype=torch.float32)
        else:
            # SOTA: å¦‚æœæ²¡æœ‰é¢„è®¡ç®—æ¡ä»¶ï¼Œè®°å½•å¹¶è¿”å›é»˜è®¤æ¡ä»¶
            # è¿™ç§æƒ…å†µä¸‹Transformåº”è¯¥å›é€€åˆ°SMILESè®¡ç®—
            return torch.tensor([0.5, 0.5], dtype=torch.float32)
    
    def normalize_conditions(self, conditions: MolecularConditions) -> MolecularConditions:
        """ğŸ”¥ ç»Ÿä¸€çš„æ¡ä»¶æ ‡å‡†åŒ–æ–¹æ¡ˆï¼Œä¸è®­ç»ƒæ•°æ®ç”Ÿæˆä¿æŒä¸€è‡´"""
        # ğŸš¨ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ä¸æ•°æ®ç”Ÿæˆä¸€è‡´çš„SAåå‘å½’ä¸€åŒ–
        # è®­ç»ƒæ•°æ®ä½¿ç”¨: (10-SA)/9ï¼Œé«˜å€¼=æ˜“åˆæˆ
        # è¿™é‡Œä¹Ÿå¿…é¡»ä½¿ç”¨ç›¸åŒçš„é€»è¾‘
        sa_raw = conditions.sa
        if sa_raw < 1.0:
            sa_raw = 1.0
        elif sa_raw > 10.0:
            sa_raw = 10.0
        sa_normalized = (10.0 - sa_raw) / 9.0  # åå‘å½’ä¸€åŒ–ï¼Œä¸è®­ç»ƒæ•°æ®ä¸€è‡´

        return MolecularConditions(
            qed=conditions.qed,  # QEDå·²ç»åœ¨0-1èŒƒå›´å†…
            sa=sa_normalized  # ğŸš¨ ä¿®å¤ï¼šä½¿ç”¨åå‘å½’ä¸€åŒ–
        )

    @staticmethod
    def denormalize_sa(sa_normalized: float) -> float:
        """ğŸš¨ ä¿®å¤ï¼šåæ ‡å‡†åŒ–SAå€¼ï¼Œä½¿ç”¨ä¸è®­ç»ƒæ•°æ®ä¸€è‡´çš„åå‘æ˜ å°„"""
        # è®­ç»ƒæ•°æ®ä½¿ç”¨: sa_normalized = (10 - sa_raw) / 9
        # åå‘è®¡ç®—: sa_raw = 10 - sa_normalized * 9
        return 10.0 - sa_normalized * 9.0

    @staticmethod
    def validate_target_conditions(target_qed: float, target_sa: float) -> tuple:
        """ğŸ”¥ ç»Ÿä¸€çš„ç›®æ ‡æ¡ä»¶éªŒè¯å’Œæ ‡å‡†åŒ–ï¼Œä¸è®­ç»ƒæ•°æ®ä¿æŒä¸€è‡´"""
        # QEDéªŒè¯ï¼š0-1èŒƒå›´
        validated_qed = max(0.0, min(1.0, target_qed))

        # ğŸš¨ SAéªŒè¯ï¼šä½¿ç”¨ä¸è®­ç»ƒæ•°æ®ä¸€è‡´çš„åå‘å½’ä¸€åŒ–
        if target_sa > 1.0:
            # å‡è®¾è¾“å…¥çš„æ˜¯åŸå§‹SAå€¼ï¼ˆå¦‚3.5ï¼‰ï¼Œéœ€è¦åå‘æ ‡å‡†åŒ–
            sa_raw = min(target_sa, 10.0)
            validated_sa = (10.0 - sa_raw) / 9.0  # åå‘å½’ä¸€åŒ–
            print(f"ğŸ”„ SAå€¼åå‘æ ‡å‡†åŒ–: åŸå§‹SA={target_sa} -> æ ‡å‡†åŒ–SA={validated_sa:.3f}")
            print(f"   è§£é‡Š: SA={target_sa}(è¾ƒéš¾åˆæˆ) -> æ ‡å‡†åŒ–å€¼={validated_sa:.3f}(ä½å€¼=éš¾åˆæˆ)")
        else:
            # å‡è®¾è¾“å…¥çš„å·²ç»æ˜¯æ ‡å‡†åŒ–å€¼ï¼Œä½†éœ€è¦éªŒè¯æ˜¯å¦ç¬¦åˆåå‘å½’ä¸€åŒ–é€»è¾‘
            validated_sa = max(0.0, min(1.0, target_sa))
            implied_sa_raw = 10.0 - validated_sa * 9.0
            print(f"ğŸ”„ SAå€¼éªŒè¯: æ ‡å‡†åŒ–SA={target_sa} -> å¯¹åº”åŸå§‹SAâ‰ˆ{implied_sa_raw:.1f}")

        return validated_qed, validated_sa
    
    def get_normalized_conditions(self, smiles: Union[str, List[str]]) -> Union[torch.Tensor, List[torch.Tensor]]:
        """è·å–æ ‡å‡†åŒ–çš„æ¡ä»¶å¼ é‡"""
        conditions = self.calculate_conditions(smiles, use_cache=True)
        
        if isinstance(conditions, list):
            normalized = [self.normalize_conditions(c) for c in conditions]
            return [c.to_tensor() for c in normalized]
        else:
            normalized = self.normalize_conditions(conditions)
            return normalized.to_tensor()
    
    def get_default_conditions(self) -> MolecularConditions:
        """è·å–é»˜è®¤æ¡ä»¶"""
        return MolecularConditions(qed=0.5, sa=0.5)
    
    def get_default_normalized_tensor(self) -> torch.Tensor:
        """è·å–é»˜è®¤æ ‡å‡†åŒ–æ¡ä»¶å¼ é‡"""
        default_conditions = self.get_default_conditions()
        normalized = self.normalize_conditions(default_conditions)
        return normalized.to_tensor()


def create_condition_processor(cache_dir: str = "data/condition_cache",
                             enable_multiprocessing: bool = True,
                             max_workers: Optional[int] = None,
                             dataset_name: str = "default") -> MolecularConditionProcessor:
    """åˆ›å»ºæ¡ä»¶å¤„ç†å™¨çš„å·¥å‚å‡½æ•°"""
    return MolecularConditionProcessor(
        cache_dir=cache_dir,
        enable_multiprocessing=enable_multiprocessing,
        max_workers=max_workers,
        dataset_name=dataset_name
    )


# å…¨å±€æ¡ä»¶å¤„ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼Œæ”¯æŒå¤šæ•°æ®é›†ï¼‰
_global_processors: Dict[str, MolecularConditionProcessor] = {}

def get_global_condition_processor(dataset_name: str = "default") -> MolecularConditionProcessor:
    """è·å–å…¨å±€æ¡ä»¶å¤„ç†å™¨å®ä¾‹"""
    global _global_processors
    if dataset_name not in _global_processors:
        _global_processors[dataset_name] = create_condition_processor(dataset_name=dataset_name)
    return _global_processors[dataset_name]
