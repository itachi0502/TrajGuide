"""
æ”¯æŒçš„æ¶æ„ï¼š
1. GNN (Geometric Graph Neural Network) - å½“å‰ä½¿ç”¨çš„æ¶æ„
2. Transformer (Self-Attention based) - åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶
3. MLP (Multi-Layer Perceptron) - ç®€å•çš„å…¨è¿æ¥ç½‘ç»œbaseline
4. Hybrid (GNN + Transformer) - æ··åˆæ¶æ„
5. BiLSTM (Bidirectional LSTM) - åŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ
6. GRU (Gated Recurrent Unit) - é—¨æ§å¾ªç¯å•å…ƒ
7. CNN (Convolutional Neural Network) - å·ç§¯ç¥ç»ç½‘ç»œ
8. ResNet (Residual Network) - æ®‹å·®ç½‘ç»œ

ç”¨äºæ¶ˆèå®éªŒï¼šåˆ†æä¸åŒæ¶æ„å¯¹å¼•å¯¼æ•ˆæœçš„å½±å“

Author: MolCRAFT Team
Date: 2025-10-14
"""

from typing import Optional, Tuple
import math

import torch
import torch.nn as nn
import torch.nn.functional as F

from torch_geometric.nn import radius_graph, global_mean_pool, global_max_pool
from torch_scatter import scatter_add


class SinusoidalTimeEmbedding(nn.Module):
    def __init__(self, time_emb_dim: int):
        super().__init__()
        self.time_emb_dim = time_emb_dim
        half = time_emb_dim // 2
        inv_freq = 1.0 / (10000 ** (torch.arange(0, half, dtype=torch.float32) / half))
        self.register_buffer('inv_freq', inv_freq, persistent=False)
        self.proj = nn.Sequential(
            nn.Linear(time_emb_dim, time_emb_dim),
            nn.SiLU(),
            nn.Linear(time_emb_dim, time_emb_dim)
        )

    def forward(self, t: torch.Tensor) -> torch.Tensor:
        if t.dim() == 0:
            t = t[None]
        if t.dim() == 1:
            t = t[:, None]
        sin_inp = t * self.inv_freq[None, :]
        emb = torch.cat([torch.sin(sin_inp), torch.cos(sin_inp)], dim=-1)
        if emb.shape[-1] < self.time_emb_dim:
            emb = F.pad(emb, (0, self.time_emb_dim - emb.shape[-1]))
        return self.proj(emb)


class MLP(nn.Module):
    """å¤šå±‚æ„ŸçŸ¥æœºï¼ˆæ‰€æœ‰æ¶æ„å…±äº«ï¼‰"""
    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.0):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, out_dim)
        )

    def forward(self, x):
        return self.net(x)


# ============================================================================
# æ¶æ„1: GNN (Geometric Graph Neural Network)
# ============================================================================

class GeoMPBlock(nn.Module):
    """å‡ ä½•æ¶ˆæ¯ä¼ é€’å—"""
    def __init__(self, hidden_dim: int, edge_dim: int, dropout: float = 0.0):
        super().__init__()
        self.msg_mlp = MLP(in_dim=hidden_dim*2 + edge_dim,
                           hidden_dim=hidden_dim,
                           out_dim=hidden_dim,
                           dropout=dropout)
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.ffn = MLP(in_dim=hidden_dim, hidden_dim=hidden_dim*2, out_dim=hidden_dim, dropout=dropout)
        self.ln2 = nn.LayerNorm(hidden_dim)

    def forward(self, x, edge_index, edge_feat):
        row, col = edge_index
        m_ij = self.msg_mlp(torch.cat([x[row], x[col], edge_feat], dim=-1))
        m_i = scatter_add(m_ij, row, dim=0, dim_size=x.size(0))
        x = self.ln1(x + m_i)
        x = self.ln2(x + self.ffn(x))
        return x


class GNNBackbone(nn.Module):
    """GNNéª¨å¹²ç½‘ç»œ"""
    def __init__(self, atom_types, hidden_dim, num_layers, time_emb_dim, dropout,
                 cutoff_radius=5.0, max_num_neighbors=32):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.cutoff_radius = cutoff_radius
        self.max_num_neighbors = max_num_neighbors

        # èŠ‚ç‚¹ç¼–ç 
        self.node_in = nn.Linear(atom_types, hidden_dim)

        # æ—¶é—´åµŒå…¥
        self.time_embed = SinusoidalTimeEmbedding(time_emb_dim)
        self.time_proj = nn.Linear(time_emb_dim, hidden_dim)

        # è¾¹ç¼–ç 
        self.dist_mlp = nn.Sequential(
            nn.Linear(1, hidden_dim // 2),
            nn.SiLU(),
            nn.Linear(hidden_dim // 2, hidden_dim // 2)
        )
        self.dir_mlp = nn.Sequential(
            nn.Linear(3, hidden_dim // 2),
            nn.SiLU(),
            nn.Linear(hidden_dim // 2, hidden_dim // 2)
        )
        self.edge_norm = nn.LayerNorm(hidden_dim)

        # GNNå±‚
        self.blocks = nn.ModuleList([
            GeoMPBlock(hidden_dim=hidden_dim, edge_dim=hidden_dim, dropout=dropout)
            for _ in range(num_layers)
        ])

        # å›¾æ± åŒ–
        self.pool_proj = nn.Linear(hidden_dim*2, hidden_dim)

    def build_molecular_graph(self, pos, batch):
        return radius_graph(pos, r=self.cutoff_radius, batch=batch,
                          max_num_neighbors=self.max_num_neighbors, loop=False)

    def encode_edges(self, pos, edge_index):
        row, col = edge_index
        edge_vec = pos[col] - pos[row]
        edge_dist = torch.norm(edge_vec, dim=-1, keepdim=True)
        edge_dir = edge_vec / (edge_dist + 1e-8)

        dist_feat = self.dist_mlp(edge_dist)
        dir_feat = self.dir_mlp(edge_dir)
        edge_feat = torch.cat([dist_feat, dir_feat], dim=-1)
        return self.edge_norm(edge_feat)

    def forward(self, theta_t, pos_t, t, batch):
        device = theta_t.device
        batch_safe = batch.detach().clone()

        # æ„å»ºå›¾
        edge_index = self.build_molecular_graph(pos_t, batch_safe)

        # èŠ‚ç‚¹åˆå§‹åŒ–
        theta_t = theta_t.softmax(dim=-1) if (theta_t.min() < 0) or (theta_t.max() > 1.0) else theta_t
        x = self.node_in(theta_t)

        # æ·»åŠ æ—¶é—´åµŒå…¥
        t_emb = self.time_proj(self.time_embed(t.to(device)))
        x = x + t_emb[batch_safe]

        # è¾¹ç‰¹å¾
        edge_feat = self.encode_edges(pos_t, edge_index)

        # æ¶ˆæ¯ä¼ é€’
        for blk in self.blocks:
            x = blk(x, edge_index, edge_feat)

        # å›¾æ± åŒ–
        mean_pool = global_mean_pool(x, batch_safe)
        max_pool = global_max_pool(x, batch_safe)
        graph_feat = torch.cat([mean_pool, max_pool], dim=-1)
        graph_feat = F.silu(self.pool_proj(graph_feat))

        return graph_feat


# ============================================================================
# æ¶æ„2: Transformer (Self-Attention based)
# ============================================================================

class TransformerBlock(nn.Module):
    """Transformerå—"""
    def __init__(self, hidden_dim, num_heads, dropout=0.1):
        super().__init__()
        self.attn = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 4, hidden_dim),
            nn.Dropout(dropout)
        )
        self.ln2 = nn.LayerNorm(hidden_dim)

    def forward(self, x, mask=None):
        # Self-attention
        attn_out, _ = self.attn(x, x, x, key_padding_mask=mask)
        x = self.ln1(x + attn_out)
        # FFN
        x = self.ln2(x + self.ffn(x))
        return x


class TransformerBackbone(nn.Module):
    """Transformeréª¨å¹²ç½‘ç»œ"""
    def __init__(self, atom_types, hidden_dim, num_layers, time_emb_dim, dropout, num_heads=8):
        super().__init__()
        self.hidden_dim = hidden_dim

        # èŠ‚ç‚¹ç¼–ç 
        self.node_in = nn.Linear(atom_types, hidden_dim)

        # ä½ç½®ç¼–ç ï¼ˆä½¿ç”¨3Dåæ ‡ï¼‰
        self.pos_encoder = nn.Sequential(
            nn.Linear(3, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # æ—¶é—´åµŒå…¥
        self.time_embed = SinusoidalTimeEmbedding(time_emb_dim)
        self.time_proj = nn.Linear(time_emb_dim, hidden_dim)

        # Transformerå±‚
        self.blocks = nn.ModuleList([
            TransformerBlock(hidden_dim, num_heads, dropout)
            for _ in range(num_layers)
        ])

        # å›¾æ± åŒ–
        self.pool_proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, theta_t, pos_t, t, batch):
        device = theta_t.device
        batch_safe = batch.detach().clone()

        # èŠ‚ç‚¹åˆå§‹åŒ–
        theta_t = theta_t.softmax(dim=-1) if (theta_t.min() < 0) or (theta_t.max() > 1.0) else theta_t
        x = self.node_in(theta_t)

        # æ·»åŠ ä½ç½®ç¼–ç 
        pos_emb = self.pos_encoder(pos_t)
        x = x + pos_emb

        # æ·»åŠ æ—¶é—´åµŒå…¥
        t_emb = self.time_proj(self.time_embed(t.to(device)))
        x = x + t_emb[batch_safe]

        # å°†èŠ‚ç‚¹ç‰¹å¾æŒ‰batchç»„ç»‡æˆåºåˆ—
        # ä¸ºäº†ä½¿ç”¨Transformerï¼Œéœ€è¦å°†æ¯ä¸ªåˆ†å­çš„åŸå­ç»„ç»‡æˆä¸€ä¸ªåºåˆ—
        batch_size = batch_safe.max().item() + 1
        max_atoms = scatter_add(torch.ones_like(batch_safe), batch_safe).max().item()

        # åˆ›å»ºå¡«å……çš„åºåˆ— [B, max_atoms, H]
        x_padded = torch.zeros(batch_size, max_atoms, self.hidden_dim, device=device)
        mask = torch.ones(batch_size, max_atoms, dtype=torch.bool, device=device)

        for b in range(batch_size):
            mask_b = (batch_safe == b)
            n_atoms = mask_b.sum().item()
            x_padded[b, :n_atoms] = x[mask_b]
            mask[b, :n_atoms] = False

        # Transformerå¤„ç†
        for blk in self.blocks:
            x_padded = blk(x_padded, mask)

        # å›¾æ± åŒ–ï¼ˆå¹³å‡æ± åŒ–ï¼Œå¿½ç•¥paddingï¼‰
        graph_feat = []
        for b in range(batch_size):
            n_atoms = (~mask[b]).sum().item()
            graph_feat.append(x_padded[b, :n_atoms].mean(dim=0))
        graph_feat = torch.stack(graph_feat, dim=0)
        graph_feat = F.silu(self.pool_proj(graph_feat))

        return graph_feat


# ============================================================================
# æ¶æ„3: MLP (Multi-Layer Perceptron) - Baseline
# ============================================================================

class MLPBackbone(nn.Module):
    """MLPéª¨å¹²ç½‘ç»œï¼ˆç®€å•baselineï¼‰"""
    def __init__(self, atom_types, hidden_dim, num_layers, time_emb_dim, dropout):
        super().__init__()
        self.hidden_dim = hidden_dim

        # æ—¶é—´åµŒå…¥
        self.time_embed = SinusoidalTimeEmbedding(time_emb_dim)
        self.time_proj = nn.Linear(time_emb_dim, hidden_dim)

        # èŠ‚ç‚¹ç¼–ç  + ä½ç½®ç¼–ç 
        self.node_in = nn.Linear(atom_types + 3, hidden_dim)  # theta + pos

        # MLPå±‚
        layers = []
        for _ in range(num_layers):
            layers.extend([
                nn.Linear(hidden_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.SiLU(),
                nn.Dropout(dropout)
            ])
        self.mlp = nn.Sequential(*layers)

        # å›¾æ± åŒ–
        self.pool_proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, theta_t, pos_t, t, batch):
        device = theta_t.device
        batch_safe = batch.detach().clone()

        # èŠ‚ç‚¹åˆå§‹åŒ–ï¼ˆæ‹¼æ¥thetaå’Œposï¼‰
        theta_t = theta_t.softmax(dim=-1) if (theta_t.min() < 0) or (theta_t.max() > 1.0) else theta_t
        x = torch.cat([theta_t, pos_t], dim=-1)
        x = self.node_in(x)

        # æ·»åŠ æ—¶é—´åµŒå…¥
        t_emb = self.time_proj(self.time_embed(t.to(device)))
        x = x + t_emb[batch_safe]

        # MLPå¤„ç†
        x = self.mlp(x)

        # å›¾æ± åŒ–ï¼ˆç®€å•å¹³å‡ï¼‰
        graph_feat = global_mean_pool(x, batch_safe)
        graph_feat = F.silu(self.pool_proj(graph_feat))

        return graph_feat


# ============================================================================
# æ¶æ„4: Hybrid (GNN + Transformer)
# ============================================================================

class HybridBackbone(nn.Module):
    """æ··åˆéª¨å¹²ç½‘ç»œï¼ˆGNN + Transformerï¼‰"""
    def __init__(self, atom_types, hidden_dim, num_layers, time_emb_dim, dropout,
                 cutoff_radius=5.0, max_num_neighbors=32, num_heads=8):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.cutoff_radius = cutoff_radius
        self.max_num_neighbors = max_num_neighbors

        # èŠ‚ç‚¹ç¼–ç 
        self.node_in = nn.Linear(atom_types, hidden_dim)

        # æ—¶é—´åµŒå…¥
        self.time_embed = SinusoidalTimeEmbedding(time_emb_dim)
        self.time_proj = nn.Linear(time_emb_dim, hidden_dim)

        # è¾¹ç¼–ç ï¼ˆç”¨äºGNNï¼‰
        self.dist_mlp = nn.Sequential(
            nn.Linear(1, hidden_dim // 2),
            nn.SiLU(),
            nn.Linear(hidden_dim // 2, hidden_dim // 2)
        )
        self.dir_mlp = nn.Sequential(
            nn.Linear(3, hidden_dim // 2),
            nn.SiLU(),
            nn.Linear(hidden_dim // 2, hidden_dim // 2)
        )
        self.edge_norm = nn.LayerNorm(hidden_dim)

        # ä½ç½®ç¼–ç ï¼ˆç”¨äºTransformerï¼‰
        self.pos_encoder = nn.Sequential(
            nn.Linear(3, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # æ··åˆå±‚ï¼šå‰åŠéƒ¨åˆ†GNNï¼ŒååŠéƒ¨åˆ†Transformer
        num_gnn_layers = num_layers // 2
        num_transformer_layers = num_layers - num_gnn_layers

        self.gnn_blocks = nn.ModuleList([
            GeoMPBlock(hidden_dim=hidden_dim, edge_dim=hidden_dim, dropout=dropout)
            for _ in range(num_gnn_layers)
        ])

        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(hidden_dim, num_heads, dropout)
            for _ in range(num_transformer_layers)
        ])

        # å›¾æ± åŒ–
        self.pool_proj = nn.Linear(hidden_dim*2, hidden_dim)

    def build_molecular_graph(self, pos, batch):
        return radius_graph(pos, r=self.cutoff_radius, batch=batch,
                          max_num_neighbors=self.max_num_neighbors, loop=False)

    def encode_edges(self, pos, edge_index):
        row, col = edge_index
        edge_vec = pos[col] - pos[row]
        edge_dist = torch.norm(edge_vec, dim=-1, keepdim=True)
        edge_dir = edge_vec / (edge_dist + 1e-8)

        dist_feat = self.dist_mlp(edge_dist)
        dir_feat = self.dir_mlp(edge_dir)
        edge_feat = torch.cat([dist_feat, dir_feat], dim=-1)
        return self.edge_norm(edge_feat)

    def forward(self, theta_t, pos_t, t, batch):
        device = theta_t.device
        batch_safe = batch.detach().clone()

        # èŠ‚ç‚¹åˆå§‹åŒ–
        theta_t = theta_t.softmax(dim=-1) if (theta_t.min() < 0) or (theta_t.max() > 1.0) else theta_t
        x = self.node_in(theta_t)

        # æ·»åŠ ä½ç½®ç¼–ç 
        pos_emb = self.pos_encoder(pos_t)
        x = x + pos_emb

        # æ·»åŠ æ—¶é—´åµŒå…¥
        t_emb = self.time_proj(self.time_embed(t.to(device)))
        x = x + t_emb[batch_safe]

        # ç¬¬ä¸€é˜¶æ®µï¼šGNNå¤„ç†
        edge_index = self.build_molecular_graph(pos_t, batch_safe)
        edge_feat = self.encode_edges(pos_t, edge_index)

        for blk in self.gnn_blocks:
            x = blk(x, edge_index, edge_feat)

        # ç¬¬äºŒé˜¶æ®µï¼šTransformerå¤„ç†
        batch_size = batch_safe.max().item() + 1
        max_atoms = scatter_add(torch.ones_like(batch_safe), batch_safe).max().item()

        # åˆ›å»ºå¡«å……çš„åºåˆ—
        x_padded = torch.zeros(batch_size, max_atoms, self.hidden_dim, device=device)
        mask = torch.ones(batch_size, max_atoms, dtype=torch.bool, device=device)

        for b in range(batch_size):
            mask_b = (batch_safe == b)
            n_atoms = mask_b.sum().item()
            x_padded[b, :n_atoms] = x[mask_b]
            mask[b, :n_atoms] = False

        for blk in self.transformer_blocks:
            x_padded = blk(x_padded, mask)

        # å›¾æ± åŒ–ï¼ˆç»“åˆå¹³å‡å’Œæœ€å¤§æ± åŒ–ï¼‰
        mean_pool = []
        max_pool = []
        for b in range(batch_size):
            n_atoms = (~mask[b]).sum().item()
            mean_pool.append(x_padded[b, :n_atoms].mean(dim=0))
            max_pool.append(x_padded[b, :n_atoms].max(dim=0)[0])

        mean_pool = torch.stack(mean_pool, dim=0)
        max_pool = torch.stack(max_pool, dim=0)
        graph_feat = torch.cat([mean_pool, max_pool], dim=-1)
        graph_feat = F.silu(self.pool_proj(graph_feat))

        return graph_feat



# ============================================================================
# æ¶æ„5: BiLSTM (Bidirectional LSTM)
# ============================================================================

class BiLSTMBackbone(nn.Module):

    def __init__(
        self,
        atom_types: int,
        hidden_dim: int,
        num_layers: int,
        time_emb_dim: int,
        dropout: float = 0.1
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.atom_types = atom_types

        # ğŸ”§ ä¿®å¤1ï¼šèŠ‚ç‚¹ç¼–ç ï¼ˆä½¿ç”¨ Linear ä»£æ›¿ Embeddingï¼Œä¸ GNN ä¸€è‡´ï¼‰
        self.node_in = nn.Linear(atom_types, hidden_dim)

        # ğŸ”§ ä¿®å¤2ï¼šä½ç½®ç¼–ç ï¼ˆå¢å¼ºä¸º2å±‚MLPï¼Œä¸ Transformer ä¸€è‡´ï¼‰
        self.pos_encoder = nn.Sequential(
            nn.Linear(3, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # æ—¶é—´åµŒå…¥
        self.time_embedding = SinusoidalTimeEmbedding(time_emb_dim)
        self.time_proj = nn.Linear(time_emb_dim, hidden_dim)

        # ğŸ”§ ä¿®å¤4ï¼šæ”¹ä¸ºå•å‘LSTMï¼Œé¿å…"ä½œå¼Š"
        # åŒå‘LSTMå¯ä»¥åŒæ—¶çœ‹åˆ°å‰åä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½å¼‚å¸¸å¥½
        # å•å‘LSTMæ›´ç¬¦åˆåºåˆ—å»ºæ¨¡çš„å®é™…åœºæ™¯
        self.bilstm = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,  # å•å‘ï¼Œæ‰€ä»¥hidden_sizeä¸å‡åŠ
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0.0,
            bidirectional=False  # ğŸ”¥ æ”¹ä¸ºå•å‘
        )

        # è¾“å‡ºæŠ•å½±ï¼ˆä¿æŒä¸å˜ï¼‰
        self.output_proj = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.SiLU(),
            nn.Dropout(dropout)
        )

    def forward(
        self,
        theta_t: torch.Tensor,
        pos_t: torch.Tensor,
        t: torch.Tensor,
        batch: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            theta_t: [N, K] åŸå­ç±»å‹ï¼ˆone-hotæˆ–logitsï¼‰
            pos_t: [N, 3] 3Dåæ ‡
            t: [B] æ—¶é—´æ­¥
            batch: [N] batchç´¢å¼•

        Returns:
            graph_feat: [B, hidden_dim] å›¾çº§ç‰¹å¾
        """
        device = theta_t.device
        batch_safe = batch.detach().clone()

        # ğŸ”§ ä¿®å¤3ï¼šèŠ‚ç‚¹ç¼–ç ï¼ˆä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒï¼Œä¸ GNN ä¸€è‡´ï¼‰
        # å½’ä¸€åŒ– theta_t åˆ°æ¦‚ç‡åˆ†å¸ƒ
        theta_t = theta_t.softmax(dim=-1) if (theta_t.min() < 0) or (theta_t.max() > 1.0) else theta_t
        x = self.node_in(theta_t)  # [N, hidden_dim] - ä½¿ç”¨ Linear å¤„ç†æ¦‚ç‡åˆ†å¸ƒ

        # æ·»åŠ ä½ç½®ç¼–ç 
        pos_emb = self.pos_encoder(pos_t)  # [N, hidden_dim]
        x = x + pos_emb

        # æ—¶é—´åµŒå…¥ï¼ˆå¹¿æ’­åˆ°æ‰€æœ‰èŠ‚ç‚¹ï¼‰
        t_emb = self.time_embedding(t.to(device))    # [B, time_emb_dim]
        t_emb = self.time_proj(t_emb)                # [B, hidden_dim]
        x = x + t_emb[batch_safe]                    # [N, hidden_dim]

        # å°†èŠ‚ç‚¹æŒ‰batchåˆ†ç»„ï¼Œè½¬æ¢ä¸ºåºåˆ—
        batch_size = batch.max().item() + 1
        sequences = []
        lengths = []

        for b in range(batch_size):
            mask = (batch == b)
            seq = x[mask]  # [n_atoms, hidden_dim]
            sequences.append(seq)
            lengths.append(seq.size(0))

        # Padåºåˆ—åˆ°ç›¸åŒé•¿åº¦
        max_len = max(lengths)
        padded_seqs = torch.zeros(batch_size, max_len, self.hidden_dim, device=x.device)
        for i, seq in enumerate(sequences):
            padded_seqs[i, :lengths[i]] = seq

        # ğŸ”§ ä¿®å¤5ï¼šå•å‘LSTMå¤„ç†
        # Pack padded sequence for efficiency
        packed_input = nn.utils.rnn.pack_padded_sequence(
            padded_seqs, lengths, batch_first=True, enforce_sorted=False
        )
        packed_output, (h_n, c_n) = self.bilstm(packed_input)

        # ä½¿ç”¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€ï¼ˆå•å‘ï¼‰
        # h_n: [num_layers, B, hidden_dim]
        graph_feat = h_n[-1, :, :]  # [B, hidden_dim] - å–æœ€åä¸€å±‚

        # è¾“å‡ºæŠ•å½±
        graph_feat = self.output_proj(graph_feat)

        return graph_feat


# ============================================================================
# æ¶æ„6: GRU (Gated Recurrent Unit)
# ============================================================================

class GRUBackbone(nn.Module):
    """
    GRUéª¨å¹²ç½‘ç»œ

    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨åŒå‘GRUæ•æ‰åºåˆ—ä¿¡æ¯
    - æ¯”LSTMå‚æ•°æ›´å°‘ï¼Œè®­ç»ƒæ›´å¿«
    - é€‚åˆå¤„ç†åŸå­åºåˆ—
    """
    def __init__(
        self,
        atom_types: int,
        hidden_dim: int,
        num_layers: int,
        time_emb_dim: int,
        dropout: float = 0.1
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        # èŠ‚ç‚¹ç¼–ç 
        self.node_encoder = nn.Embedding(atom_types, hidden_dim)

        # ä½ç½®ç¼–ç ï¼ˆ3Dåæ ‡ï¼‰
        self.pos_encoder = nn.Linear(3, hidden_dim)

        # æ—¶é—´åµŒå…¥
        self.time_embedding = SinusoidalTimeEmbedding(time_emb_dim)
        self.time_proj = nn.Linear(time_emb_dim, hidden_dim)

        # BiGRUå±‚
        self.bigru = nn.GRU(
            input_size=hidden_dim,
            hidden_size=hidden_dim // 2,  # åŒå‘ï¼Œæ‰€ä»¥hidden_sizeå‡åŠ
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0.0,
            bidirectional=True
        )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.SiLU(),
            nn.Dropout(dropout)
        )

    def forward(
        self,
        theta_t: torch.Tensor,
        pos_t: torch.Tensor,
        t: torch.Tensor,
        batch: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            theta_t: [N, K] åŸå­ç±»å‹ï¼ˆone-hotæˆ–logitsï¼‰
            pos_t: [N, 3] 3Dåæ ‡
            t: [B] æ—¶é—´æ­¥
            batch: [N] batchç´¢å¼•

        Returns:
            graph_feat: [B, hidden_dim] å›¾çº§ç‰¹å¾
        """
        # èŠ‚ç‚¹ç¼–ç 
        if theta_t.dim() == 2 and theta_t.size(1) > 1:
            theta_idx = theta_t.argmax(dim=-1)
        else:
            theta_idx = theta_t.long().squeeze(-1)

        x = self.node_encoder(theta_idx)  # [N, hidden_dim]
        x = x + self.pos_encoder(pos_t)   # æ·»åŠ ä½ç½®ä¿¡æ¯

        # æ—¶é—´åµŒå…¥ï¼ˆå¹¿æ’­åˆ°æ‰€æœ‰èŠ‚ç‚¹ï¼‰
        t_emb = self.time_embedding(t)    # [B, time_emb_dim]
        t_emb = self.time_proj(t_emb)     # [B, hidden_dim]
        x = x + t_emb[batch]              # [N, hidden_dim]

        # å°†èŠ‚ç‚¹æŒ‰batchåˆ†ç»„ï¼Œè½¬æ¢ä¸ºåºåˆ—
        batch_size = batch.max().item() + 1
        sequences = []
        lengths = []

        for b in range(batch_size):
            mask = (batch == b)
            seq = x[mask]  # [n_atoms, hidden_dim]
            sequences.append(seq)
            lengths.append(seq.size(0))

        # Padåºåˆ—åˆ°ç›¸åŒé•¿åº¦
        max_len = max(lengths)
        padded_seqs = torch.zeros(batch_size, max_len, self.hidden_dim, device=x.device)
        for i, seq in enumerate(sequences):
            padded_seqs[i, :lengths[i]] = seq

        # BiGRUå¤„ç†
        # Pack padded sequence for efficiency
        packed_input = nn.utils.rnn.pack_padded_sequence(
            padded_seqs, lengths, batch_first=True, enforce_sorted=False
        )
        packed_output, h_n = self.bigru(packed_input)

        # ä½¿ç”¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€ï¼ˆå‰å‘å’Œåå‘æ‹¼æ¥ï¼‰
        # h_n: [num_layers * 2, B, hidden_dim // 2]
        h_forward = h_n[-2, :, :]   # [B, hidden_dim // 2]
        h_backward = h_n[-1, :, :]  # [B, hidden_dim // 2]
        graph_feat = torch.cat([h_forward, h_backward], dim=-1)  # [B, hidden_dim]

        # è¾“å‡ºæŠ•å½±
        graph_feat = self.output_proj(graph_feat)

        return graph_feat




# ============================================================================
# æ¶æ„7: CNN (Convolutional Neural Network)
# ============================================================================

class CNNBackbone(nn.Module):
    """
    CNNéª¨å¹²ç½‘ç»œ

    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨1Då·ç§¯å¤„ç†åŸå­åºåˆ—
    - æ•æ‰å±€éƒ¨æ¨¡å¼
    - å‚æ•°å…±äº«ï¼Œè®¡ç®—é«˜æ•ˆ
    """
    def __init__(
        self,
        atom_types: int,
        hidden_dim: int,
        num_layers: int,
        time_emb_dim: int,
        dropout: float = 0.1,
        kernel_size: int = 3
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.kernel_size = kernel_size

        # èŠ‚ç‚¹ç¼–ç 
        self.node_encoder = nn.Embedding(atom_types, hidden_dim)

        # ä½ç½®ç¼–ç ï¼ˆ3Dåæ ‡ï¼‰
        self.pos_encoder = nn.Linear(3, hidden_dim)

        # æ—¶é—´åµŒå…¥
        self.time_embedding = SinusoidalTimeEmbedding(time_emb_dim)
        self.time_proj = nn.Linear(time_emb_dim, hidden_dim)

        # 1Då·ç§¯å±‚
        self.conv_layers = nn.ModuleList()
        for i in range(num_layers):
            self.conv_layers.append(
                nn.Sequential(
                    nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=kernel_size//2),
                    nn.BatchNorm1d(hidden_dim),
                    nn.SiLU(),
                    nn.Dropout(dropout)
                )
            )

        # å…¨å±€æ± åŒ–
        self.global_pool = nn.AdaptiveMaxPool1d(1)

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.SiLU(),
            nn.Dropout(dropout)
        )

    def forward(
        self,
        theta_t: torch.Tensor,
        pos_t: torch.Tensor,
        t: torch.Tensor,
        batch: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            theta_t: [N, K] åŸå­ç±»å‹ï¼ˆone-hotæˆ–logitsï¼‰
            pos_t: [N, 3] 3Dåæ ‡
            t: [B] æ—¶é—´æ­¥
            batch: [N] batchç´¢å¼•

        Returns:
            graph_feat: [B, hidden_dim] å›¾çº§ç‰¹å¾
        """
        # èŠ‚ç‚¹ç¼–ç 
        if theta_t.dim() == 2 and theta_t.size(1) > 1:
            theta_idx = theta_t.argmax(dim=-1)
        else:
            theta_idx = theta_t.long().squeeze(-1)

        x = self.node_encoder(theta_idx)  # [N, hidden_dim]
        x = x + self.pos_encoder(pos_t)   # æ·»åŠ ä½ç½®ä¿¡æ¯

        # æ—¶é—´åµŒå…¥ï¼ˆå¹¿æ’­åˆ°æ‰€æœ‰èŠ‚ç‚¹ï¼‰
        t_emb = self.time_embedding(t)    # [B, time_emb_dim]
        t_emb = self.time_proj(t_emb)     # [B, hidden_dim]
        x = x + t_emb[batch]              # [N, hidden_dim]

        # å°†èŠ‚ç‚¹æŒ‰batchåˆ†ç»„ï¼Œè½¬æ¢ä¸ºåºåˆ—
        batch_size = batch.max().item() + 1
        sequences = []
        lengths = []

        for b in range(batch_size):
            mask = (batch == b)
            seq = x[mask]  # [n_atoms, hidden_dim]
            sequences.append(seq)
            lengths.append(seq.size(0))

        # Padåºåˆ—åˆ°ç›¸åŒé•¿åº¦
        max_len = max(lengths)
        padded_seqs = torch.zeros(batch_size, max_len, self.hidden_dim, device=x.device)
        for i, seq in enumerate(sequences):
            padded_seqs[i, :lengths[i]] = seq

        # è½¬æ¢ä¸ºCNNè¾“å…¥æ ¼å¼: [B, C, L]
        x = padded_seqs.transpose(1, 2)  # [B, hidden_dim, max_len]

        # å·ç§¯å±‚
        for conv_layer in self.conv_layers:
            x = conv_layer(x) + x  # æ®‹å·®è¿æ¥

        # å…¨å±€æ± åŒ–
        x = self.global_pool(x).squeeze(-1)  # [B, hidden_dim]

        # è¾“å‡ºæŠ•å½±
        graph_feat = self.output_proj(x)

        return graph_feat


# ============================================================================
# æ¶æ„8: ResNet (Residual Network)
# ============================================================================

class ResidualBlock(nn.Module):
    """æ®‹å·®å—"""
    def __init__(self, hidden_dim: int, dropout: float = 0.1):
        super().__init__()
        self.block = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )
        self.activation = nn.SiLU()

    def forward(self, x):
        return self.activation(x + self.block(x))


class ResNetBackbone(nn.Module):
    """
    ResNetéª¨å¹²ç½‘ç»œ

    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨æ®‹å·®è¿æ¥ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
    - å¯ä»¥è®­ç»ƒæ›´æ·±çš„ç½‘ç»œ
    - é€‚åˆå¤æ‚çš„ç‰¹å¾æå–
    """
    def __init__(
        self,
        atom_types: int,
        hidden_dim: int,
        num_layers: int,
        time_emb_dim: int,
        dropout: float = 0.1
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        # èŠ‚ç‚¹ç¼–ç 
        self.node_encoder = nn.Embedding(atom_types, hidden_dim)

        # ä½ç½®ç¼–ç ï¼ˆ3Dåæ ‡ï¼‰
        self.pos_encoder = nn.Linear(3, hidden_dim)

        # æ—¶é—´åµŒå…¥
        self.time_embedding = SinusoidalTimeEmbedding(time_emb_dim)
        self.time_proj = nn.Linear(time_emb_dim, hidden_dim)

        # æ®‹å·®å—
        self.res_blocks = nn.ModuleList([
            ResidualBlock(hidden_dim, dropout) for _ in range(num_layers)
        ])

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.SiLU(),
            nn.Dropout(dropout)
        )

    def forward(
        self,
        theta_t: torch.Tensor,
        pos_t: torch.Tensor,
        t: torch.Tensor,
        batch: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            theta_t: [N, K] åŸå­ç±»å‹ï¼ˆone-hotæˆ–logitsï¼‰
            pos_t: [N, 3] 3Dåæ ‡
            t: [B] æ—¶é—´æ­¥
            batch: [N] batchç´¢å¼•

        Returns:
            graph_feat: [B, hidden_dim] å›¾çº§ç‰¹å¾
        """
        # èŠ‚ç‚¹ç¼–ç 
        if theta_t.dim() == 2 and theta_t.size(1) > 1:
            theta_idx = theta_t.argmax(dim=-1)
        else:
            theta_idx = theta_t.long().squeeze(-1)

        x = self.node_encoder(theta_idx)  # [N, hidden_dim]
        x = x + self.pos_encoder(pos_t)   # æ·»åŠ ä½ç½®ä¿¡æ¯

        # æ—¶é—´åµŒå…¥ï¼ˆå¹¿æ’­åˆ°æ‰€æœ‰èŠ‚ç‚¹ï¼‰
        t_emb = self.time_embedding(t)    # [B, time_emb_dim]
        t_emb = self.time_proj(t_emb)     # [B, hidden_dim]
        x = x + t_emb[batch]              # [N, hidden_dim]

        # æ®‹å·®å—å¤„ç†
        for res_block in self.res_blocks:
            x = res_block(x)

        # å›¾æ± åŒ–ï¼ˆmean + maxï¼‰
        batch_size = batch.max().item() + 1
        mean_pool = global_mean_pool(x, batch)  # [B, hidden_dim]
        max_pool = global_max_pool(x, batch)    # [B, hidden_dim]
        graph_feat = mean_pool + max_pool       # [B, hidden_dim]

        # è¾“å‡ºæŠ•å½±
        graph_feat = self.output_proj(graph_feat)

        return graph_feat


# ============================================================================
# ç»Ÿä¸€æ¥å£ï¼šå¤šæ¶æ„æ¡ä»¶å¼•å¯¼ç½‘ç»œ
# ============================================================================

class MultiArchGuidanceNetwork(nn.Module):
    """
    æ”¯æŒå¤šç§æ¶æ„çš„æ¡ä»¶å¼•å¯¼ç½‘ç»œ

    Args:
        architecture: 'gnn', 'transformer', 'mlp', 'hybrid', 'bilstm', 'gru', 'cnn', 'resnet'
        atom_types: åŸå­ç±»å‹æ•°é‡
        hidden_dim: éšè—å±‚ç»´åº¦
        num_layers: ç½‘ç»œå±‚æ•°
        time_emb_dim: æ—¶é—´åµŒå…¥ç»´åº¦
        condition_dim: æ¡ä»¶ç»´åº¦ï¼ˆQED, SAï¼‰
        dropout: Dropoutç‡
        cutoff_radius: GNNçš„æˆªæ–­åŠå¾„
        max_num_neighbors: GNNçš„æœ€å¤§é‚»å±…æ•°
        num_heads: Transformerçš„æ³¨æ„åŠ›å¤´æ•°
        kernel_size: CNNçš„å·ç§¯æ ¸å¤§å°
    """
    def __init__(
        self,
        architecture: str = 'gnn',
        atom_types: int = 100,
        hidden_dim: int = 256,
        num_layers: int = 4,
        time_emb_dim: int = 64,
        condition_dim: int = 2,
        dropout: float = 0.1,
        cutoff_radius: float = 5.0,
        max_num_neighbors: int = 32,
        num_heads: int = 8,
        kernel_size: int = 3,
    ):
        super().__init__()
        self.architecture = architecture.lower()
        self.condition_dim = condition_dim

        # é€‰æ‹©éª¨å¹²ç½‘ç»œ
        if self.architecture == 'gnn':
            self.backbone = GNNBackbone(
                atom_types, hidden_dim, num_layers, time_emb_dim, dropout,
                cutoff_radius, max_num_neighbors
            )
        elif self.architecture == 'transformer':
            self.backbone = TransformerBackbone(
                atom_types, hidden_dim, num_layers, time_emb_dim, dropout, num_heads
            )
        elif self.architecture == 'mlp':
            self.backbone = MLPBackbone(
                atom_types, hidden_dim, num_layers, time_emb_dim, dropout
            )
        elif self.architecture == 'hybrid':
            self.backbone = HybridBackbone(
                atom_types, hidden_dim, num_layers, time_emb_dim, dropout,
                cutoff_radius, max_num_neighbors, num_heads
            )
        elif self.architecture == 'bilstm':
            self.backbone = BiLSTMBackbone(
                atom_types, hidden_dim, num_layers, time_emb_dim, dropout
            )
        elif self.architecture == 'gru':
            self.backbone = GRUBackbone(
                atom_types, hidden_dim, num_layers, time_emb_dim, dropout
            )
        elif self.architecture == 'cnn':
            self.backbone = CNNBackbone(
                atom_types, hidden_dim, num_layers, time_emb_dim, dropout, kernel_size
            )
        elif self.architecture == 'resnet':
            self.backbone = ResNetBackbone(
                atom_types, hidden_dim, num_layers, time_emb_dim, dropout
            )
        else:
            raise ValueError(f"Unknown architecture: {architecture}. "
                           f"Choose from ['gnn', 'transformer', 'mlp', 'hybrid', "
                           f"'bilstm', 'gru', 'cnn', 'resnet']")

        # é¢„æµ‹å¤´ï¼ˆæ‰€æœ‰æ¶æ„å…±äº«ï¼‰
        self.mu_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, condition_dim),
            nn.Sigmoid()  # å½’ä¸€åŒ–åˆ°[0,1]
        )
        self.sigma_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, condition_dim)
        )
        self.softplus = nn.Softplus()

    def forward(
        self,
        theta_t: torch.Tensor,
        pos_t: torch.Tensor,
        t: torch.Tensor,
        batch: torch.Tensor,
        edge_index: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        Returns:
            mu: [B, condition_dim] é¢„æµ‹çš„å‡å€¼
            sigma: [B, condition_dim] é¢„æµ‹çš„æ ‡å‡†å·®ï¼ˆæ­£æ•°ï¼‰
        """
        with torch.enable_grad():
            # éª¨å¹²ç½‘ç»œæå–ç‰¹å¾
            graph_feat = self.backbone(theta_t, pos_t, t, batch)

            # é¢„æµ‹å¤´
            mu = self.mu_head(graph_feat)
            raw_sigma = self.sigma_head(graph_feat)
            sigma = self.softplus(raw_sigma) + 1e-3
            sigma = sigma.clamp(min=1e-3, max=0.5)

            return mu, sigma


# ============================================================================
# å·¥å‚å‡½æ•°
# ============================================================================

def create_multi_arch_guidance_network(config: Optional[dict] = None) -> MultiArchGuidanceNetwork:
    """
    åˆ›å»ºå¤šæ¶æ„æ¡ä»¶å¼•å¯¼ç½‘ç»œ

    Args:
        config: é…ç½®å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š
            - architecture: 'gnn', 'transformer', 'mlp', 'hybrid'
            - atom_types: åŸå­ç±»å‹æ•°é‡
            - hidden_dim: éšè—å±‚ç»´åº¦
            - num_layers: ç½‘ç»œå±‚æ•°
            - time_emb_dim: æ—¶é—´åµŒå…¥ç»´åº¦
            - condition_dim: æ¡ä»¶ç»´åº¦
            - dropout: Dropoutç‡
            - cutoff_radius: GNNçš„æˆªæ–­åŠå¾„
            - max_num_neighbors: GNNçš„æœ€å¤§é‚»å±…æ•°
            - num_heads: Transformerçš„æ³¨æ„åŠ›å¤´æ•°

    Returns:
        MultiArchGuidanceNetworkå®ä¾‹
    """
    default_config = {
        'architecture': 'gnn',
        'atom_types': 100,
        'hidden_dim': 256,
        'num_layers': 4,
        'time_emb_dim': 64,
        'condition_dim': 2,
        'dropout': 0.1,
        'cutoff_radius': 5.0,
        'max_num_neighbors': 32,
        'num_heads': 8,
    }

    if config is not None:
        default_config.update(config)

    return MultiArchGuidanceNetwork(**default_config)


def get_architecture_info(architecture: str) -> dict:
    """
    è·å–æ¶æ„ä¿¡æ¯

    Args:
        architecture: 'gnn', 'transformer', 'mlp', 'hybrid', 'bilstm', 'gru', 'cnn', 'resnet'

    Returns:
        åŒ…å«æ¶æ„æè¿°çš„å­—å…¸
    """
    info = {
        'gnn': {
            'name': 'Geometric Graph Neural Network',
            'description': 'åŸºäºå‡ ä½•å›¾ç¥ç»ç½‘ç»œï¼Œä½¿ç”¨æ¶ˆæ¯ä¼ é€’æ•æ‰3Dç©ºé—´ç»“æ„',
            'strengths': ['æ•æ‰å±€éƒ¨å‡ ä½•ä¿¡æ¯', 'å‚æ•°æ•ˆç‡é«˜', 'å¯¹åˆ†å­å›¾ç»“æ„æ•æ„Ÿ'],
            'weaknesses': ['æ„Ÿå—é‡å—é™äºæˆªæ–­åŠå¾„', 'éš¾ä»¥æ•æ‰é•¿ç¨‹ç›¸äº’ä½œç”¨']
        },
        'transformer': {
            'name': 'Self-Attention Transformer',
            'description': 'åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¨å±€å»ºæ¨¡åŸå­é—´ç›¸äº’ä½œç”¨',
            'strengths': ['å…¨å±€æ„Ÿå—é‡', 'æ•æ‰é•¿ç¨‹ç›¸äº’ä½œç”¨', 'çµæ´»çš„æ³¨æ„åŠ›æ¨¡å¼'],
            'weaknesses': ['è®¡ç®—å¤æ‚åº¦é«˜O(NÂ²)', 'å‚æ•°é‡å¤§', 'å¯èƒ½å¿½ç•¥å±€éƒ¨å‡ ä½•']
        },
        'mlp': {
            'name': 'Multi-Layer Perceptron',
            'description': 'ç®€å•çš„å…¨è¿æ¥ç½‘ç»œï¼Œä½œä¸ºbaseline',
            'strengths': ['ç®€å•é«˜æ•ˆ', 'å‚æ•°å°‘', 'è®­ç»ƒå¿«'],
            'weaknesses': ['æ— æ³•æ•æ‰åŸå­é—´ç›¸äº’ä½œç”¨', 'å¿½ç•¥å›¾ç»“æ„', 'æ€§èƒ½è¾ƒå·®']
        },
        'hybrid': {
            'name': 'Hybrid GNN + Transformer',
            'description': 'æ··åˆæ¶æ„ï¼šå…ˆç”¨GNNæ•æ‰å±€éƒ¨å‡ ä½•ï¼Œå†ç”¨Transformerå»ºæ¨¡å…¨å±€',
            'strengths': ['ç»“åˆGNNå’ŒTransformerä¼˜åŠ¿', 'å±€éƒ¨+å…¨å±€ä¿¡æ¯', 'æ€§èƒ½æœ€ä¼˜'],
            'weaknesses': ['å‚æ•°é‡æœ€å¤§', 'è®¡ç®—å¤æ‚åº¦é«˜', 'è®­ç»ƒæ—¶é—´é•¿']
        },
        'bilstm': {
            'name': 'Bidirectional LSTM',
            'description': 'åŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼Œæ•æ‰åºåˆ—çš„å‰å‘å’Œåå‘ä¾èµ–',
            'strengths': ['æ•æ‰é•¿æœŸä¾èµ–', 'åŒå‘ä¿¡æ¯æµ', 'é€‚åˆåºåˆ—å»ºæ¨¡'],
            'weaknesses': ['è®­ç»ƒé€Ÿåº¦æ…¢', 'éš¾ä»¥å¹¶è¡ŒåŒ–', 'å¯èƒ½å¿½ç•¥3Då‡ ä½•']
        },
        'gru': {
            'name': 'Gated Recurrent Unit',
            'description': 'é—¨æ§å¾ªç¯å•å…ƒï¼Œæ¯”LSTMå‚æ•°æ›´å°‘çš„åºåˆ—æ¨¡å‹',
            'strengths': ['æ¯”LSTMå‚æ•°å°‘', 'è®­ç»ƒæ›´å¿«', 'æ•æ‰åºåˆ—ä¾èµ–'],
            'weaknesses': ['éš¾ä»¥å¹¶è¡ŒåŒ–', 'å¯èƒ½å¿½ç•¥3Då‡ ä½•', 'é•¿åºåˆ—æ€§èƒ½ä¸‹é™']
        },
        'cnn': {
            'name': 'Convolutional Neural Network',
            'description': '1Då·ç§¯ç¥ç»ç½‘ç»œï¼Œæ•æ‰å±€éƒ¨æ¨¡å¼',
            'strengths': ['å‚æ•°å…±äº«', 'è®¡ç®—é«˜æ•ˆ', 'æ•æ‰å±€éƒ¨æ¨¡å¼', 'æ˜“äºå¹¶è¡ŒåŒ–'],
            'weaknesses': ['æ„Ÿå—é‡å—é™', 'éš¾ä»¥æ•æ‰é•¿ç¨‹ä¾èµ–', 'å¯èƒ½å¿½ç•¥å…¨å±€ç»“æ„']
        },
        'resnet': {
            'name': 'Residual Network',
            'description': 'æ®‹å·®ç½‘ç»œï¼Œä½¿ç”¨æ®‹å·®è¿æ¥è®­ç»ƒæ›´æ·±çš„ç½‘ç»œ',
            'strengths': ['ç¼“è§£æ¢¯åº¦æ¶ˆå¤±', 'å¯è®­ç»ƒæ›´æ·±ç½‘ç»œ', 'ç‰¹å¾æå–èƒ½åŠ›å¼º'],
            'weaknesses': ['å‚æ•°é‡è¾ƒå¤§', 'å¯èƒ½å¿½ç•¥å›¾ç»“æ„', 'éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®']
        }
    }

    return info.get(architecture.lower(), {'name': 'Unknown', 'description': 'Unknown architecture'})


