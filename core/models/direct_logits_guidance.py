import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple


class DirectLogitsGuidance:

    
    def __init__(self):

        self.atom_indices = {
            'H': 0, 'C': 1, 'N': 2, 'O': 3,
            'F': 4, 'P': 5, 'S': 6, 'Cl': 7
        }
        

        self.qed_guidance_weights = torch.tensor([
            -5.0,   
            -3.0,   
            +10.0,  
            +10.0,  
            +3.0,   
            -5.0,   
            -5.0, 
            -3.0   
        ])
        

        self.sa_guidance_weights = torch.tensor([
            +5.0, 
            +10.0,  
            -5.0, 
            -5.0,   
            -8.0,   
            -10.0, 
            -10.0,  
            -8.0 
        ])
    
    def compute_guidance_logits(
        self,
        theta_t: torch.Tensor,              # [N, K] 
        current_properties: torch.Tensor,   # [B, 2] 
        target_properties: torch.Tensor,    # [B, 2] 
        batch_ligand: torch.Tensor,         # [N] 
        guidance_scale: float = 2.0,
        current_time: float = 0.5
    ) -> torch.Tensor:
        """
        è®¡ç®—å¼•å¯¼logits
        
        Returns:
            guidance_logits: [N, K] è¦æ·»åŠ åˆ°log(theta)çš„logits
        """
        device = theta_t.device
        N, K = theta_t.shape
        batch_size = torch.unique(batch_ligand).numel()
        
        # ç¡®ä¿K=8
        if K != 8:
            raise ValueError(f"Expected K=8, got K={K}")
        
        # è®¡ç®—æ€§è´¨å·®è·
        qed_gap = target_properties[:, 0] - current_properties[:, 0]  # [B]
        sa_gap = target_properties[:, 1] - current_properties[:, 1]    # [B]
        

        qed_gap = torch.relu(qed_gap)
        sa_gap = torch.relu(sa_gap)   
        
  

        qed_amplification = torch.exp(5.0 * qed_gap) - 1.0  # [B]
        sa_amplification = torch.exp(5.0 * sa_gap) - 1.0    # [B]
        

        if current_time < 0.3:
            time_factor = 2.0  # æ—©æœŸï¼šå¼ºå¼•å¯¼
        elif current_time < 0.7:
            time_factor = 1.5  # ä¸­æœŸï¼šä¸­ç­‰å¼•å¯¼
        else:
            time_factor = 1.0  # åæœŸï¼šç»´æŒå¼•å¯¼
        
        # æ„é€ å¼•å¯¼logits
        guidance_logits = torch.zeros(N, K, device=device)
        
        for b in range(batch_size):
            mask = (batch_ligand == b)
            
            # QEDå¼•å¯¼
            qed_contribution = (
                qed_gap[b].item() * 
                qed_amplification[b].item() * 
                self.qed_guidance_weights.to(device)
            )
            
            # SAå¼•å¯¼
            sa_contribution = (
                sa_gap[b].item() * 
                sa_amplification[b].item() * 
                self.sa_guidance_weights.to(device)
            )
            

            total_guidance = qed_contribution + sa_contribution
            
            # åº”ç”¨åˆ°è¯¥åˆ†å­çš„æ‰€æœ‰åŸå­
            guidance_logits[mask] = total_guidance.unsqueeze(0)
        
        # åº”ç”¨guidance_scaleå’Œtime_factor
        guidance_logits = guidance_logits * guidance_scale * time_factor
        
        return guidance_logits
    
    def apply_direct_guidance(
        self,
        theta_prime: torch.Tensor,          # [N, K] æœªå¼•å¯¼çš„ä¿¡å¿µçŠ¶æ€
        current_properties: torch.Tensor,   # [B, 2] å½“å‰é¢„æµ‹çš„QED, SA
        target_properties: torch.Tensor,    # [B, 2] ç›®æ ‡QED, SA
        batch_ligand: torch.Tensor,         # [N] æ‰¹æ¬¡ç´¢å¼•
        guidance_scale: float = 2.0,
        current_time: float = 0.5
    ) -> Dict:
        """
        åº”ç”¨ç›´æ¥logitså¼•å¯¼
        
        Returns:
            dictåŒ…å«:
                - guided_theta: [N, K] å¼•å¯¼åçš„ä¿¡å¿µçŠ¶æ€
                - guidance_logits: [N, K] åº”ç”¨çš„å¼•å¯¼logits
                - guidance_strength: float å®é™…å¼•å¯¼å¼ºåº¦
        """
        # è®¡ç®—å¼•å¯¼logits
        guidance_logits = self.compute_guidance_logits(
            theta_prime, current_properties, target_properties,
            batch_ligand, guidance_scale, current_time
        )
        
        # ğŸ”¥ å…³é”®ï¼šç›´æ¥ä¿®æ”¹logits
        log_theta_prime = torch.log(theta_prime + 1e-10)
        log_guided = log_theta_prime + guidance_logits
        
        # å½’ä¸€åŒ–
        guided_theta = F.softmax(log_guided, dim=-1)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        guidance_strength = torch.abs(guidance_logits).mean().item()
        max_guidance = torch.abs(guidance_logits).max().item()
        
        # è®¡ç®—KLæ•£åº¦
        kl_div = torch.sum(
            guided_theta * (torch.log(guided_theta + 1e-10) - torch.log(theta_prime + 1e-10)),
            dim=-1
        ).mean().item()
        
        return {
            'guided_theta': guided_theta,
            'guidance_logits': guidance_logits,
            'guidance_strength': guidance_strength,
            'max_guidance': max_guidance,
            'kl_divergence': kl_div
        }


class HybridGuidanceIntegrator:

    
    def __init__(
        self,
        base_integrator,  
        use_direct_guidance: bool = True,
        direct_guidance_weight: float = 0.7  
    ):
        self.base_integrator = base_integrator
        self.use_direct_guidance = use_direct_guidance
        self.direct_guidance_weight = direct_guidance_weight
        
        if use_direct_guidance:
            self.direct_guidance = DirectLogitsGuidance()
        else:
            self.direct_guidance = None

    
    def apply_multiplicative_guidance(
        self,
        theta_prime: torch.Tensor,
        pos_t: torch.Tensor,
        t: torch.Tensor,
        batch_ligand: torch.Tensor,
        target_conditions: torch.Tensor,
        guidance_scale: float = 1.0,
        alpha_h: float = None
    ) -> dict:
        # 1. è°ƒç”¨åŸºç¡€å‡ ä½•å¼•å¯¼
        base_result = self.base_integrator.apply_multiplicative_guidance(
            theta_prime, pos_t, t, batch_ligand, target_conditions,
            guidance_scale, alpha_h
        )
        
        if not self.use_direct_guidance or self.direct_guidance is None:
            return base_result
        
        # 2. ä¼°ç®—å½“å‰æ€§è´¨
        try:
            # ä½¿ç”¨åŸºç¡€é›†æˆå™¨çš„å¼•å¯¼æ¨¡å‹é¢„æµ‹å½“å‰æ€§è´¨
            device = theta_prime.device
            batch_size = torch.unique(batch_ligand).numel()
            
            if t.numel() == 1:
                t_batch = t.expand(batch_size)
            elif t.size(0) == theta_prime.size(0):
                t_batch = torch.zeros(batch_size, device=device, dtype=t.dtype)
                for b in range(batch_size):
                    mask = (batch_ligand == b)
                    if mask.any():
                        t_batch[b] = t[mask][0]
            else:
                t_batch = t
            
            with torch.no_grad():
                pred_mu, pred_sigma = self.base_integrator.guidance_model(
                    theta_t=theta_prime,
                    pos_t=pos_t,
                    t=t_batch,
                    batch=batch_ligand
                )
            
            current_properties = pred_mu
        except:
            # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
            batch_size = torch.unique(batch_ligand).numel()
            current_properties = torch.tensor(
                [[0.5, 0.5]] * batch_size,
                device=theta_prime.device
            )
        
        # 3. åº”ç”¨ç›´æ¥logitså¼•å¯¼
        current_time = t[0].item() if t.numel() > 0 else 0.5
        
        direct_result = self.direct_guidance.apply_direct_guidance(
            theta_prime, current_properties, target_conditions,
            batch_ligand, guidance_scale, current_time
        )
        
        # 4. æ··åˆä¸¤ç§å¼•å¯¼ç»“æœ
        # ä½¿ç”¨åŠ æƒå¹³å‡ï¼ˆåœ¨æ¦‚ç‡ç©ºé—´ï¼‰
        base_theta = base_result['guided_theta']
        direct_theta = direct_result['guided_theta']
        
        w_direct = self.direct_guidance_weight
        w_base = 1.0 - w_direct
        
        mixed_theta = w_base * base_theta + w_direct * direct_theta
        
        # é‡æ–°å½’ä¸€åŒ–
        mixed_theta = mixed_theta / (mixed_theta.sum(dim=-1, keepdim=True) + 1e-10)
        
        # 5. æ›´æ–°ç»“æœ
        base_result['guided_theta'] = mixed_theta
        base_result['hybrid_guidance'] = True
        base_result['direct_guidance_strength'] = direct_result['guidance_strength']
        base_result['direct_guidance_kl'] = direct_result['kl_divergence']
        
        return base_result


def create_hybrid_guidance_integrator(
    guidance_model_path: str,
    device: str = 'cuda',
    use_direct_guidance: bool = True,
    direct_guidance_weight: float = 0.7,
    **kwargs
):
    # 1. åˆ›å»ºåŸºç¡€å‡ ä½•å¼•å¯¼é›†æˆå™¨
    from core.models.geometric_guidance_integration import create_geometric_guidance_integrator
    
    base_integrator = create_geometric_guidance_integrator(
        guidance_model_path=guidance_model_path,
        device=device,
        **kwargs
    )
    
    if base_integrator is None:
        return None
    
    # 2. åŒ…è£…ä¸ºæ··åˆå¼•å¯¼é›†æˆå™¨
    hybrid_integrator = HybridGuidanceIntegrator(
        base_integrator=base_integrator,
        use_direct_guidance=use_direct_guidance,
        direct_guidance_weight=direct_guidance_weight
    )
    
    
    return hybrid_integrator

