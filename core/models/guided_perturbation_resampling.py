import torch
import torch.nn.functional as F
import numpy as np
from typing import Optional, Tuple, List, Dict
from rdkit import Chem
from rdkit.Chem import QED
from rdkit.Contrib.SA_Score import sascorer


def sa_norm_from_rdkit(sa_raw: float) -> float:

    v = float(sa_raw)
    v = max(1.0, min(10.0, v))
    return (10.0 - v) / 9.0


class GuidedPerturbation:

    
    def __init__(
        self,
        guidance_model,
        perturbation_strength: float = 0.15,
        guided_ratio: float = 0.7,
        random_ratio: float = 0.3,
        adaptive_strength: bool = True,
        verbose: bool = True,
    ):

        self.guidance_model = guidance_model
        self.perturbation_strength = perturbation_strength
        self.guided_ratio = guided_ratio
        self.random_ratio = random_ratio
        self.adaptive_strength = adaptive_strength
        self.verbose = verbose
        
        # å½’ä¸€åŒ–æƒé‡
        total_ratio = guided_ratio + random_ratio
        self.guided_ratio = guided_ratio / total_ratio
        self.random_ratio = random_ratio / total_ratio
    
    def compute_guided_direction(
        self,
        theta_t: torch.Tensor,
        pos_t: torch.Tensor,
        batch_ligand: torch.Tensor,
        target_qed: float,
        target_sa: float,
        t: float = 1.0,
    ) -> Tuple[torch.Tensor, Dict]:

        device = theta_t.device
        batch_size = batch_ligand.max().item() + 1
        
        # å‡†å¤‡æ—¶é—´å¼ é‡
        t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.float32)
        
        # å‡†å¤‡ç›®æ ‡æ¡ä»¶
        target_conditions = torch.tensor(
            [[target_qed, target_sa]] * batch_size,
            device=device,
            dtype=torch.float32
        )  # [B, 2]
        
   
        with torch.no_grad():
            pred_mu, pred_sigma = self.guidance_model(
                theta_t=theta_t,
                pos_t=pos_t,
                t=t_tensor,
                batch=batch_ligand,
            )

        

        delta_qed = target_conditions[:, 0] - pred_mu[:, 0]  # [B]
        delta_sa = target_conditions[:, 1] - pred_mu[:, 1]   # [B]
       
        eps = 1e-6
        grad_qed = delta_qed / (pred_sigma[:, 0] ** 2 + eps)  # [B]
        grad_sa = delta_sa / (pred_sigma[:, 1] ** 2 + eps)    # [B]
        
        # åˆå¹¶QEDå’ŒSAçš„æ¢¯åº¦
        delta_log_prob = grad_qed + grad_sa  # [B]
        
        # åˆ†é…åˆ°æ¯ä¸ªåŽŸå­
        delta_log_prob_per_atom = delta_log_prob[batch_ligand]  # [N]
        
        # æ‰©å±•åˆ°åŽŸå­ç±»åž‹ç»´åº¦
        guided_direction = delta_log_prob_per_atom.unsqueeze(-1)  # [N, 1]
        
        # è¯Šæ–­ä¿¡æ¯
        info = {
            'pred_qed': pred_mu[:, 0].mean().item(),
            'pred_sa': pred_mu[:, 1].mean().item(),
            'delta_qed': delta_qed.mean().item(),
            'delta_sa': delta_sa.mean().item(),
            'guidance_strength': delta_log_prob.abs().mean().item(),
        }
        
        return guided_direction, info
    
    def perturb_theta(
        self,
        theta_t: torch.Tensor,
        pos_t: torch.Tensor,
        batch_ligand: torch.Tensor,
        target_qed: float,
        target_sa: float,
        current_qed: Optional[float] = None,
        current_sa: Optional[float] = None,
        t: float = 1.0,
    ) -> Tuple[torch.Tensor, Dict]:

        # 1. è®¡ç®—å¼•å¯¼æ–¹å‘
        guided_direction, guidance_info = self.compute_guided_direction(
            theta_t=theta_t,
            pos_t=pos_t,
            batch_ligand=batch_ligand,
            target_qed=target_qed,
            target_sa=target_sa,
            t=t,
        )
        
        # 2. è‡ªé€‚åº”è°ƒæ•´æ‰°åŠ¨å¼ºåº¦
        strength = self.perturbation_strength
        if self.adaptive_strength and current_qed is not None and current_sa is not None:
            # æ ¹æ®åå·®å¤§å°è°ƒæ•´å¼ºåº¦
            qed_deviation = abs(current_qed - target_qed)
            sa_deviation = abs(current_sa - target_sa)
            total_deviation = qed_deviation + sa_deviation
            
            # åå·®è¶Šå¤§ï¼Œæ‰°åŠ¨å¼ºåº¦è¶Šå¤§ï¼ˆä½†æœ‰ä¸Šé™ï¼‰
            strength = self.perturbation_strength * (1.0 + total_deviation)
            strength = min(strength, self.perturbation_strength * 3.0)  # æœ€å¤š3å€
            
            if self.verbose:
                print(f"   ðŸŽ¯ è‡ªé€‚åº”æ‰°åŠ¨å¼ºåº¦: {strength:.4f} (åŸºç¡€={self.perturbation_strength:.4f}, åå·®={total_deviation:.4f})")
        
        # 3. åœ¨logç©ºé—´è¿›è¡Œæ‰°åŠ¨
        eps = 1e-10
        log_theta = torch.log(theta_t + eps)
        
        # 3.1 å¼•å¯¼æ€§æ‰°åŠ¨
        guided_perturbation = strength * self.guided_ratio * guided_direction
        
        # 3.2 éšæœºå™ªå£°ï¼ˆæŽ¢ç´¢æ€§ï¼‰
        random_noise = torch.randn_like(log_theta) * (strength * self.random_ratio)
        
        # 3.3 æ··åˆ
        log_theta_perturbed = log_theta + guided_perturbation + random_noise
        
        # 4. å½’ä¸€åŒ–
        theta_perturbed = F.softmax(log_theta_perturbed, dim=-1)
        
        # 5. è¯Šæ–­ä¿¡æ¯
        info = {
            **guidance_info,
            'perturbation_strength': strength,
            'guided_ratio': self.guided_ratio,
            'random_ratio': self.random_ratio,
            'theta_change': (theta_perturbed - theta_t).abs().mean().item(),
        }
        

        
        return theta_perturbed, info


class PartialResampler:

    
    def __init__(
        self,
        resample_last_steps: int = 20,
        max_attempts: int = 3,
        verbose: bool = True,
    ):

        self.resample_last_steps = resample_last_steps
        self.max_attempts = max_attempts
        self.verbose = verbose
    
    def resample(
        self,
        dynamics_model,
        theta_chain: List[torch.Tensor],
        pos_chain: List[torch.Tensor],
        perturbation_calculator: GuidedPerturbation,
        batch_ligand: torch.Tensor,
        protein_pos: torch.Tensor,
        protein_v: torch.Tensor,
        batch_protein: torch.Tensor,
        target_qed: float,
        target_sa: float,
        current_qed: float,
        current_sa: float,
        guidance_scale: float = 2.0,
        **sampling_kwargs
    ) -> Tuple[Optional[Chem.Mol], Dict]:

        return None, {'status': 'not_implemented'}

