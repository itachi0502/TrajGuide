import argparse
import os
import sys
import logging
from pathlib import Path
from datetime import datetime
import json

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from core.models.geometric_guidance_network import GeometricGuidanceNetwork, create_geometric_guidance_network
from core.models.guidance_architectures import (
    MultiArchGuidanceNetwork,
    create_multi_arch_guidance_network,
    get_architecture_info
)
from core.datasets.geometric_guidance_dataset import create_geometric_guidance_dataloader
from core.models.sbdd4train import SBDD4Train
from core.config.config import Config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def load_backbone_model(config_path: str, checkpoint_path: str = None):
    """
    åŠ è½½éª¨æ¶æ¨¡å‹ï¼Œç”¨äºè®¡ç®—theta_tå’Œpos_t

    è¿™ç¡®ä¿äº†è®­ç»ƒæ•°æ®ä¸éª¨æ¶æ¨¡å‹ä½¿ç”¨å®Œå…¨ä¸€è‡´çš„çŠ¶æ€è¡¨ç¤º
    """
    try:
        logger.info(f"ğŸ“‚ åŠ è½½éª¨æ¶æ¨¡å‹é…ç½®: {config_path}")

        #  ä½¿ç”¨æ­£ç¡®çš„Configç±»
        config = Config(config_path)

        if checkpoint_path and os.path.exists(checkpoint_path):


            backbone_model = SBDD4Train.load_from_checkpoint(
                checkpoint_path,
                config=config,
                strict=False
            )
            logger.info(" æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        else:
            backbone_model = SBDD4Train(config=config)

        backbone_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        return backbone_model

    except Exception as e:
        logger.error(f" åŠ è½½éª¨æ¶æ¨¡å‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def compute_enhanced_loss(pred_mu, pred_sigma, target_properties, clamp=(1e-3, 0.5)):

    # è£å‰ªé¢„æµ‹çš„æ ‡å‡†å·®åˆ°åˆç†èŒƒå›´
    pred_sigma_clamped = torch.clamp(pred_sigma, clamp[0], clamp[1])

    # 1. NLLæŸå¤±ï¼ˆæ¦‚ç‡å»ºæ¨¡ï¼‰
    # NLL = log(Ïƒ) + 0.5 * (target - Î¼)Â² / ÏƒÂ²
    qed_nll = torch.log(pred_sigma_clamped[:, 0]) + \
              0.5 * ((target_properties[:, 0] - pred_mu[:, 0]) ** 2) / (pred_sigma_clamped[:, 0] ** 2)
    sa_nll = torch.log(pred_sigma_clamped[:, 1]) + \
             0.5 * ((target_properties[:, 1] - pred_mu[:, 1]) ** 2) / (pred_sigma_clamped[:, 1] ** 2)

    # 2. MSEæŸå¤±ï¼ˆç›´æ¥ç›‘ç£ï¼‰
    qed_mse = F.mse_loss(pred_mu[:, 0], target_properties[:, 0])
    sa_mse = F.mse_loss(pred_mu[:, 1], target_properties[:, 1])

    # 3. æ–¹å·®æ­£åˆ™åŒ–ï¼ˆé˜²æ­¢æ–¹å·®è¿‡å¤§æˆ–è¿‡å°ï¼Œç›®æ ‡æ–¹å·®0.1ï¼‰
    sigma_reg = torch.mean((pred_sigma_clamped - 0.1) ** 2)

    # ç»„åˆæŸå¤±
    qed_loss = qed_nll.mean() + 0.5 * qed_mse  # NLL + 0.5*MSE
    sa_loss = sa_nll.mean() + 0.5 * sa_mse

    # æ€»æŸå¤±ï¼šQEDæƒé‡2.0ï¼ˆå¢åŠ ï¼‰ï¼ŒSAæƒé‡1.0ï¼Œæ–¹å·®æ­£åˆ™åŒ–0.1
    total_loss = 2.0 * qed_loss + 1.0 * sa_loss + 0.1 * sigma_reg

    return total_loss, qed_loss, sa_loss



def train_epoch(model, dataloader, optimizer, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch - ä½¿ç”¨ç®€å•çš„NLLæŸå¤±"""
    model.train()
    total_loss = 0.0
    total_qed_loss = 0.0
    total_sa_loss = 0.0
    num_batches = len(dataloader)

    pbar = tqdm(dataloader, desc=f'Epoch {epoch}')
    for batch_idx, batch in enumerate(pbar):
        theta_i = batch['theta_i'].to(device) 
        alpha_i = batch['alpha_i'].to(device) 
        pos_t = batch['pos_t'].to(device)
        target_properties = batch['target_properties'].to(device) 
        batch_ligand = batch['batch_ligand'].to(device)

        pred_mu, pred_sigma = model(theta_i, pos_t, alpha_i, batch_ligand)

        # è®¡ç®—å¢å¼ºæŸå¤±ï¼ˆNLL + MSE + æ–¹å·®æ­£åˆ™åŒ–ï¼‰
        loss, qed_loss, sa_loss = compute_enhanced_loss(
            pred_mu, pred_sigma, target_properties
        )

        optimizer.zero_grad()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        total_loss += loss.item()
        total_qed_loss += qed_loss.item()
        total_sa_loss += sa_loss.item()

        pbar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'QED_Loss': f'{qed_loss.item():.4f}',
            'SA_Loss': f'{sa_loss.item():.4f}'
        })

    avg_loss = total_loss / num_batches
    avg_qed_loss = total_qed_loss / num_batches
    avg_sa_loss = total_sa_loss / num_batches

    return avg_loss, avg_qed_loss, avg_sa_loss


def evaluate_model(model, dataloader, device, split_name='Val'):
    model.eval()
    total_loss = 0.0
    total_qed_loss = 0.0
    total_sa_loss = 0.0
    num_batches = len(dataloader)

    all_pred_mu = []
    all_target_properties = []

    with torch.no_grad():
        pbar = tqdm(dataloader, desc=f'{split_name} Eval')
        for batch in pbar:
            theta_i = batch['theta_i'].to(device)
            alpha_i = batch['alpha_i'].to(device)
            pos_t = batch['pos_t'].to(device)
            target_properties = batch['target_properties'].to(device)
            batch_ligand = batch['batch_ligand'].to(device)

            pred_mu, pred_sigma = model(theta_i, pos_t, alpha_i, batch_ligand)

            loss, qed_loss, sa_loss = compute_enhanced_loss(
                pred_mu, pred_sigma, target_properties
            )

            total_loss += loss.item()
            total_qed_loss += qed_loss.item()
            total_sa_loss += sa_loss.item()
            all_pred_mu.append(pred_mu.cpu())
            all_target_properties.append(target_properties.cpu())

            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})

    avg_loss = total_loss / num_batches
    avg_qed_loss = total_qed_loss / num_batches
    avg_sa_loss = total_sa_loss / num_batches
    all_pred_mu = torch.cat(all_pred_mu, dim=0)  # [N, 2]
    all_target_properties = torch.cat(all_target_properties, dim=0)  # [N, 2]

    qed_mse = F.mse_loss(all_pred_mu[:, 0], all_target_properties[:, 0]).item()
    sa_mse = F.mse_loss(all_pred_mu[:, 1], all_target_properties[:, 1]).item()

    metrics = {
        'total_loss': avg_loss,
        'qed_loss': avg_qed_loss,
        'sa_loss': avg_sa_loss,
        'qed_mse': qed_mse,
        'sa_mse': sa_mse
    }

    return metrics


def save_checkpoint(model, optimizer, epoch, metrics, output_dir):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics
    }
    
    checkpoint_path = output_dir / f'geometric_guidance_epoch_{epoch:03d}.pt'
    torch.save(checkpoint, checkpoint_path)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    best_path = output_dir / 'geometric_guidance_best.pt'
    torch.save(checkpoint, best_path)
    
    logger.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")


def main():
    parser = argparse.ArgumentParser(description="è®­ç»ƒå‡ ä½•æ„ŸçŸ¥æ¡ä»¶å¼•å¯¼ç½‘ç»œ")
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®ç›®å½•")
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--config", type=str, help="éª¨æ¶æ¨¡å‹é…ç½®æ–‡ä»¶")
    parser.add_argument("--backbone_ckpt", type=str, help="éª¨æ¶æ¨¡å‹æ£€æŸ¥ç‚¹")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=20, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-5, help="æƒé‡è¡°å‡")

    # æ¨¡å‹å‚æ•°
    parser.add_argument("--architecture", type=str, default="gnn",
                       choices=['gnn', 'transformer', 'mlp', 'hybrid', 'bilstm', 'gru', 'cnn', 'resnet'],
                       help="å¼•å¯¼ç½‘ç»œæ¶æ„: gnn, transformer, mlp, hybrid, bilstm, gru, cnn, resnet")
    parser.add_argument("--hidden_dim", type=int, default=256, help="éšè—ç»´åº¦")
    parser.add_argument("--num_layers", type=int, default=4, help="ç½‘ç»œå±‚æ•°")
    parser.add_argument("--num_heads", type=int, default=8, help="Transformeræ³¨æ„åŠ›å¤´æ•°")
    parser.add_argument("--cutoff_radius", type=float, default=5.0, help="GNNæˆªæ–­åŠå¾„")
    parser.add_argument("--max_num_neighbors", type=int, default=32, help="GNNæœ€å¤§é‚»å±…æ•°")
    parser.add_argument("--kernel_size", type=int, default=3, help="CNNå·ç§¯æ ¸å¤§å°")

    # å…¶ä»–å‚æ•°
    parser.add_argument("--device", type=str, default="auto", help="è®¾å¤‡")
    parser.add_argument("--num_workers", type=int, default=0, help="æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--max_samples", type=int, help="æœ€å¤§æ ·æœ¬æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)
    
    logger.info(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    arch_info = get_architecture_info(args.architecture)
    logger.info("=" * 80)
    logger.info(f" {args.architecture.upper()}")
    logger.info(f" {arch_info['name']}")
    logger.info(f" {arch_info['description']}")
    logger.info(f"advantage:")
    for strength in arch_info['strengths']:
        logger.info(f"{strength}")
    logger.info(f"disadvantage:")
    for weakness in arch_info['weaknesses']:
        logger.info(f"{weakness}")
    logger.info("=" * 80)


    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    backbone_model = None
    if args.config:
        backbone_model = load_backbone_model(args.config, args.backbone_ckpt)
        if backbone_model:
            backbone_model = backbone_model.to(device)
    

    atom_types = 8  
    total_theta_dim = 8  

    if backbone_model is not None:
        K = getattr(backbone_model.dynamics, 'num_classes', 8) 
        KH = getattr(backbone_model.dynamics, 'num_charge', 0)  
        KA = getattr(backbone_model.dynamics, 'num_aromatic', 0)  

        total_theta_dim = K + KH + KA



    model_config = {
        'architecture': args.architecture, 
        'atom_types': total_theta_dim, 
        'hidden_dim': args.hidden_dim,
        'num_layers': args.num_layers,
        'num_heads': args.num_heads,
        'condition_dim': 2, 
        'cutoff_radius': args.cutoff_radius,
        'max_num_neighbors': args.max_num_neighbors,
        'kernel_size': args.kernel_size,  
        'dropout': 0.1,
        'time_emb_dim': 64
    }


    model = create_multi_arch_guidance_network(model_config)
    model = model.to(device)

    train_loader = create_geometric_guidance_dataloader(
        args.data_dir, 'train', args.batch_size, backbone_model, 
        args.num_workers, shuffle=True, max_samples=args.max_samples
    )
    
    val_loader = create_geometric_guidance_dataloader(
        args.data_dir, 'val', args.batch_size, backbone_model,
        args.num_workers, shuffle=False, max_samples=args.max_samples
    )
    

    test_loader = create_geometric_guidance_dataloader(
        args.data_dir, 'test', args.batch_size, backbone_model,
        args.num_workers, shuffle=False, max_samples=args.max_samples
    )
    

    sample_times = []
    num_batches_to_sample = min(20, len(train_loader))  

    train_iter = iter(train_loader)
    for _ in range(num_batches_to_sample):
        try:
            batch = next(train_iter)
            batch_times = batch['alpha_i'].flatten().tolist()
            sample_times.extend(batch_times)
        except StopIteration:
            break

    if len(sample_times) > 0:
        sample_times = torch.tensor(sample_times)
        time_mean = sample_times.mean().item()
        time_std = sample_times.std().item()

        backbone_times = torch.linspace(0, 1, len(sample_times))
        backbone_mean = backbone_times.mean().item()
        backbone_std = backbone_times.std().item()



    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)


    scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=args.epochs, eta_min=1e-6
    )
    scheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=5, factor=0.5, verbose=True
    )

    best_val_loss = float('inf')
    train_history = []

    patience = 15
    patience_counter = 0

    logger.info("training...")
    logger.info(f"   Early Stopping: patience={patience}")

    for epoch in range(1, args.epochs + 1):
        # è®­ç»ƒ
        train_loss, train_qed_loss, train_sa_loss = train_epoch(
            model, train_loader, optimizer, device, epoch
        )
        
        # éªŒè¯
        val_metrics = evaluate_model(model, val_loader, device, 'Val')

        scheduler_cosine.step()
        scheduler_plateau.step(val_metrics['total_loss'])

        epoch_info = {
            'epoch': epoch,
            'train_loss': train_loss,
            'train_qed_loss': train_qed_loss,
            'train_sa_loss': train_sa_loss,
            'val_metrics': val_metrics,
            'lr': optimizer.param_groups[0]['lr']
        }
        train_history.append(epoch_info)
        
        logger.info(f"Epoch {epoch:3d} | "
                   f"Train Loss: {train_loss:.4f} | "
                   f"Val Loss: {val_metrics['total_loss']:.4f} | "
                   f"QED MSE: {val_metrics['qed_mse']:.4f} | "
                   f"SA MSE: {val_metrics['sa_mse']:.4f} | "
                   f"LR: {optimizer.param_groups[0]['lr']:.2e}")
        

        if val_metrics['total_loss'] < best_val_loss:
            best_val_loss = val_metrics['total_loss']
            patience_counter = 0  
            save_checkpoint(model, optimizer, epoch, val_metrics, output_dir)
            logger.info(f"æ–°çš„æœ€ä½³æ¨¡å‹! éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        else:
            patience_counter += 1
            logger.info(f"   éªŒè¯æŸå¤±æœªæ”¹è¿› ({patience_counter}/{patience})")

            # Early stopping
            if patience_counter >= patience:
                logger.info(f" Early stoppingè§¦å‘! åœ¨epoch {epoch}åœæ­¢è®­ç»ƒ")
                logger.info(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                break

        if epoch % 10 == 0:
            logger.info("è¯¦ç»†æ€§èƒ½åˆ†æ:")
            logger.info(f"   QEDé¢„æµ‹MSE: {val_metrics['qed_mse']:.6f} (ç›®æ ‡: <0.01)")
            logger.info(f"   SAé¢„æµ‹MSE: {val_metrics['sa_mse']:.6f} (ç›®æ ‡: <0.01)")
            logger.info(f"   æ€»ä½“NLLæŸå¤±: {val_metrics['total_loss']:.6f}")

            # æ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆ
            if train_loss < val_metrics['total_loss'] * 0.5:
                logger.warning("âš ï¸ å¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆï¼Œè€ƒè™‘å¢åŠ æ­£åˆ™åŒ–æˆ–æ—©åœ")
    
    logger.info("åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    test_metrics = evaluate_model(model, test_loader, device, 'Test')
    
    logger.info("æœ€ç»ˆæµ‹è¯•ç»“æœ:")
    logger.info(f"   æ€»æŸå¤±: {test_metrics['total_loss']:.4f}")
    logger.info(f"   QED MSE: {test_metrics['qed_mse']:.4f}")
    logger.info(f"   SA MSE: {test_metrics['sa_mse']:.4f}")
    
    final_results = {
        'architecture': args.architecture, 
        'architecture_info': arch_info,
        'train_history': train_history,
        'test_metrics': test_metrics,
        'model_config': model_config,
        'training_args': vars(args)
    }

    results_file = output_dir / f'training_results_{args.architecture}.json'
    with open(results_file, 'w') as f:
        json.dump(final_results, f, indent=2)

    logger.info(f"è®­ç»ƒå®Œæˆï¼ç»“æœä¿å­˜è‡³: {output_dir}")
    logger.info(f"   æ¶æ„: {args.architecture.upper()}")
    logger.info(f"   ç»“æœæ–‡ä»¶: {results_file}")

    logger.info("=" * 80)
    logger.info("è®­ç»ƒå®Œæˆæ€»ç»“:")
    logger.info(f"   æ¶æ„: {args.architecture.upper()}")
    logger.info(f"   æ—¶é—´æ­¥åˆ†å¸ƒä¸€è‡´æ€§: å·²éªŒè¯")
    logger.info(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    logger.info(f"   QEDé¢„æµ‹ç²¾åº¦: MSE={test_metrics['qed_mse']:.6f}")
    logger.info(f"   SAé¢„æµ‹ç²¾åº¦: MSE={test_metrics['sa_mse']:.6f}")
    logger.info(f"   æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()
